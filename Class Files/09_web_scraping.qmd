---
title: "Web Scraping in R"
format:
  html: default
editor_options: 
  chunk_output_type: console
---
  
You can download this .qmd file from [here](https://github.com/proback/264_fall_2024/blob/main/09_web_scraping.qmd).  Just hit the Download Raw File button.

Credit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(polite)
library(sf)
library(maps)
library(viridis)
library(leaflet)
library(htmltools)

# Starter step: install SelectorGadget (https://selectorgadget.com/) in your browser

```


# Using rvest for web scraping

Please see `08_table_scraping.qmd` for a preview of web scraping techniques when no API exists, along with ethical considerations when scraping data.  In this file, we will turn to scenarios when the webpage contains data of interest, but it is not already in table form.  


## Recall the four steps to scraping data with functions in the `rvest` library:

0. `robotstxt::paths_allowed()` Check if the website allows scraping, and then make sure we scrape "politely"
1. `read_html()`.  Input the URL containing the data and turn the html code into an XML file (another markup format that's easier to work with).
2. `html_nodes()`.  Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css="table" for tables.)
3. `html_text()`.  Extract content of interest from nodes.  Might also use `html_table()` etc.


## More scraping ethics

### `robots.txt`

`robots.txt` is a file that some websites will publish to clarify what can and cannot be scraped and other constraints about scraping. When a website publishes this file, this we need to comply with the information in it for moral and legal reasons.

We will look through the information in [this tutorial](https://www.zenrows.com/blog/robots-txt-web-scraping) and apply this to the [NIH robots.txt file](https://www.nih.gov/robots.txt).

From our investigation of the NIH `robots.txt`, we learn:

- `User-agent: *`: Anyone is allowed to scrape
- `Crawl-delay: 2`: Need to wait 2 seconds between each page scraped
- No `Visit-time` entry: no restrictions on time of day that scraping is allowed
- No `Request-rate` entry: no restrictions on simultaneous requests
- No mention of `?page=`, `news-events`, `news-releases`, or `https://science.education.nih.gov/` in the `Disallow` sections. (This is what we want to scrape today.)

### robotstxt package

We can also use functions from the [`robotstxt` package](https://cran.r-project.org/web/packages/robotstxt/), which was built to download and parse robots.txt files ([more info](https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html)).  Specifically, the `paths_allowed()` function can check if a bot has permission to access certain pages.


## A timeout to preview some technical ideas

### HTML structure

HTML (hypertext markup language) is the formatting language used to create webpages. We can see the core parts of HTML from the [rvest vignette](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html).

### Finding CSS Selectors

In order to gather information from a webpage, we must learn the language used to identify patterns of specific information. For example, on the [NIH News Releases page](https://www.nih.gov/news-events/news-releases), we can see that the data is represented in a consistent pattern of image + title + abstract.

We will identify data in a web page using a pattern matching language called [CSS Selectors](https://css-tricks.com/how-css-selectors-work/) that can refer to specific patterns in HTML, the language used to write web pages.

For example:

- Selecting by tag:
    - `"a"` selects all hyperlinks in a webpage ("a" represents "anchor" links in HTML)
    - `"p"` selects all paragraph elements

- Selecting by ID and class:
    - `".description"` selects all elements with `class` equal to "description"
        - The `.` at the beginning is what signifies `class` selection.
        - This is one of the most common CSS selectors for scraping because in HTML, the `class` attribute is extremely commonly used to format webpage elements. (Any number of HTML elements can have the same `class`, which is not true for the `id` attribute.)
    - `"#mainTitle"` selects the SINGLE element with **id** equal to "mainTitle"
        - The `#` at the beginning is what signifies `id` selection.

```html
<p class="title">Title of resource 1</p>
<p class="description">Description of resource 1</p>

<p class="title">Title of resource 2</p>
<p class="description">Description of resource 2</p>
```

**Warning**: Websites change often! So if you are going to scrape a lot of data, it is probably worthwhile to save and date a copy of the website. Otherwise, you may return after some time and your scraping code will include all of the wrong CSS selectors.

### SelectorGadget

Although you can [learn how to use CSS Selectors by hand]([CSS Selectors](https://css-tricks.com/how-css-selectors-work/)), we will use a shortcut by installing the [Selector Gadget](http://selectorgadget.com/) tool.

- There is a version available for Chrome--add it to Chrome via the [Chome Web Store](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb).
    - Make sure to pin the extension to the menu bar. (Click the 3 dots > Extensions > Manage extensions. Click the "Details" button under SelectorGadget and toggle the "Pin to toolbar" option.)
- There is also a version that can be saved as a bookmark in the browser--see [here](https://rvest.tidyverse.org/articles/selectorgadget.html).

You might watch the Selector Gadget [tutorial video](http://selectorgadget.com/).


## Case Study: NIH News Releases

Our goal is to build a data frame with the article title, publication date, and abstract text for the 50 most recent NIH news releases.  

Head over to the [NIH News Releases page](https://www.nih.gov/news-events/news-releases). Click the Selector Gadget extension icon or bookmark button. As you mouse over the webpage, different parts will be highlighted in orange. Click on the title (but not the live link portion!) of the first news release. You'll notice that the Selector Gadget information in the lower right describes what you clicked on.  (If SelectorGadget ever highlights too much in green, you can click on portions that you do not want to turn them red.)

Scroll through the page to verify that only the information you intend (the description paragraph) is selected. The selector panel shows the CSS selector (`.teaser-title`) and the number of matches for that CSS selector (10). (You may have to be careful with your clicking--there are two overlapping boxes, and clicking on the link of the title can lead to the CSS selector of "a".)


**[Pause to Ponder:]** Repeat the process above to find the correct selectors for the following fields. Make sure that each matches 10 results:

- The publication date

> .date-display-single

- The article abstract paragraph (which will also include the publication date)

> .teaser-description


### Retrieving Data Using `rvest` and CSS Selectors

Now that we have identified CSS selectors for the information we need, let's fetch the data using the `rvest` package similarly to our approach in `08_table_scraping.qmd`.

```{r}
# check that scraping is allowed (Step 0)
robotstxt::paths_allowed("https://www.nih.gov/news-events/news-releases")

# Step 1: Download the HTML and turn it into an XML file with read_html()
nih <- read_html("https://www.nih.gov/news-events/news-releases")
```

Finding the exact node (e.g. ".teaser-title") is the tricky part.  Among all the html code used to produce a webpage, where do you go to grab the content of interest?  This is where SelectorGadget comes to the rescue!

```{r}
# Step 2: Extract specific nodes with html_nodes()
title_temp <- html_nodes(nih, ".teaser-title")
title_temp

# Step 3: Extract content from nodes with html_text(), html_name(), 
#    html_attrs(), html_children(), html_table(), etc.
# Usually will still need to do some stringr adjustments
title_vec <- html_text(title_temp)
title_vec
```

You can also write this altogether with a pipe:

```{r}
robotstxt::paths_allowed("https://www.nih.gov/news-events/news-releases")

read_html("https://www.nih.gov/news-events/news-releases") |>
  html_nodes(".teaser-title") |>
  html_text()
```

And finally we wrap the 4 steps above into the `bow` and `scrape` functions from the `polite` package:

```{r}
session <- bow("https://www.nih.gov/news-events/news-releases", force = TRUE)

nih_title <- scrape(session) |>
  html_nodes(".teaser-title") |>
  html_text()
nih_title
```


### Putting multiple columns of data together.

Now repeat the process above to extract the publication date and the abstract.

```{r}
nih_pubdate <- scrape(session) |>
  html_nodes(".date-display-single") |>
  html_text()
nih_pubdate

nih_description <- scrape(session) |>
  html_nodes(".teaser-description") |>
  html_text()
nih_description
```

Combine these extracted variables into a single tibble.  Make sure the variables are formatted correctly - e.g. `pubdate` has `date` type, `description` does not contain the `pubdate`, etc.

```{r}
# use tibble() to put multiple columns together into a tibble
nih_top10 <- tibble(title = nih_title, 
                    pubdate = nih_pubdate, 
                    description = nih_description)
nih_top10

# now clean the data
nih_top10 <- nih_top10 |>
  mutate(pubdate = mdy(pubdate),
         description = str_trim(str_replace(description, ".*\\n", "")))
nih_top10
```

NOW - continue this process to build a tibble with the most recent 50 NIH news releases, which will require that you iterate over 5 webpages!  You should write at least one function, and you will need iteration--use both a `for` loop and appropriate `map_()` functions from `purrr`. Some additional hints:

- Mouse over the page buttons at the very bottom of the news home page to see what the URLs look like.
- Include `Sys.sleep(2)` in your function to respect the `Crawl-delay: 2` in the NIH `robots.txt` file.
- Recall that `bind_rows()` from `dplyr` takes a list of data frames and stacks them on top of each other.

**[Pause to Ponder:]** Create a function to scrape a single NIH press release page by filling missing pieces labeled `???`:

```{r}
#| eval: FALSE

# Helper function to reduce html_nodes() |> html_text() code duplication
get_text_from_page <- function(page, css_selector) {
  page |>
  html_nodes(css_selector) |>
  html_text()
}

# Main function to scrape and tidy desired attributes
scrape_page <- function(url) {
    Sys.sleep(2)
    page <- read_html(url)
    article_titles <- get_text_from_page(page, ".teaser-title")
    article_dates <- get_text_from_page(page, ".date-display-single")
    article_dates <- mdy(article_dates)
    article_description <- get_text_from_page(page, ".teaser-description")
    article_description <- str_trim(str_replace(article_description, 
                                                ".*\\n", 
                                                "")
                                    )
    
    tibble(
      title = article_titles,
      pub_dates = article_dates, 
      description = article_description
    )
}

scrape_page("https://www.nih.gov/news-events/news-releases")
```


**[Pause to Ponder:]** Use a for loop over the first 5 pages:

```{r}
#| eval: FALSE

pages <- vector("list", length = 5)

for (i in 1:5) {
    Sys.sleep(2)
    base_url <- "https://www.nih.gov/news-events/news-releases"
    if (i==1) {
        url <- base_url
    } else {
        url <- str_c(base_url, "?page=", i-1)
    }
    pages[[i]] <- scrape_page(url)
}

df_articles <- bind_rows(pages)
head(df_articles)
```

**[Pause to Ponder:]** Use map functions in the purrr package:

```{r}
#| eval: FALSE

# Create a character vector of URLs for the first 5 pages
base_url <- "???"
urls_all_pages <- c(base_url, str_c(???))

pages2 <- purrr::map(???)
df_articles2 <- bind_rows(pages2)
head(df_articles2)
```


## On Your Own

1. Go to https://www.bestplaces.net and search for Minneapolis, Minnesota.  This is a site some people use when comparing cities they might consider working in and/or moving to.  Using SelectorGadget, extract the following pieces of information from the Minneapolis page:

- property crime (on a scale from 0 to 100)
>> .col-4 > div:nth-child(1)

- minimum income required for a single person to live comfortably
>> .text-center+ .text-center div:nth-child(1)

- average monthly rent for a 2-bedroom apartment
>> .col-3~ .col-3+ .col-3 .mb-2

- the "about" paragraph (the very first paragraph above "Location Details")
>> .ms-3:nth-child(3)

```{r}
scrape_page <- function(url) {
    Sys.sleep(2)
    page <- read_html(url)
    article_titles <- get_text_from_page(page, ".teaser-title")
    article_dates <- get_text_from_page(page, ".date-display-single")
    article_dates <- mdy(article_dates)
    article_description <- get_text_from_page(page, ".teaser-description")
    article_description <- str_trim(str_replace(article_description, 
                                                ".*\\n", 
                                                "")
                                    )
    
    tibble(
      title = article_titles,
      pub_dates = article_dates, 
      description = article_description
    )
}
```


```{r}
get_text_from_page <- function(page, css_selector) {
  page |>
  html_nodes(css_selector) |>
  html_text()
}

page <- read_html("https://www.bestplaces.net/city/minnesota/minneapolis")

prop_crime <- get_text_from_page(page, ".col-4 > div:nth-child(1)")
medium_single_income <- get_text_from_page(page, ".text-center+ .text-center div:nth-child(1)")
avg_month_rent <- get_text_from_page(page, ".col-3~ .col-3+ .col-3 .mb-2")
about <- get_text_from_page(page, ".ms-3:nth-child(3)")
```

```{r}
minneapolis <- tibble(
      crime = prop_crime,
      min_income_single = medium_single_income, 
      rent_2br = avg_month_rent,
      about = about 
    )
```

```{r}
minneapolis |>
  mutate(crime = str_remove(crime, " / 100"),
         crime = as.integer(crime),
         min_income_single = parse_number(min_income_single),
         rent_2br = parse_number(rent_2br))
```


2. Write a function called `scrape_bestplaces()` with arguments for `state` and `city`.  When you run, for example, `scrape_bestplaces("minnesota", "minneapolis")`, the output should be a 1 x 6 tibble with columns for `state`, `city`, `crime`, `min_income_single`, `rent_2br`, and `about`.

```{r}
scrape_bestplaces <- function(state, city){
url <- str_c("https://www.bestplaces.net/city/", state, "/", city)
page <- read_html(url)

prop_crime <- get_text_from_page(page, ".col-4 > div:nth-child(1)")
medium_single_income <- get_text_from_page(page, ".text-center+ .text-center div:nth-child(1)")
avg_month_rent <- get_text_from_page(page, ".col-3~ .col-3+ .col-3 .mb-2")
about <- get_text_from_page(page, ".ms-3:nth-child(3)")

tibble <- tibble(
      state = state,
      city = city,
      crime = prop_crime,
      min_income_single = medium_single_income, 
      rent_2br = avg_month_rent,
      about = about
    )

tibble |>
  mutate(crime = str_remove(crime, " / 100"),
         crime = as.integer(crime),
         min_income_single = parse_number(min_income_single),
         rent_2br = parse_number(rent_2br))
}
```

```{r}
scrape_bestplaces("minnesota", "minneapolis")
scrape_bestplaces("oregon", "portland")
```


3. Create a 5 x 6 tibble by running `scrape_bestplaces()` 5 times with 5 cities you are interested in.  You might have to combine tibbles using `bind_rows()`.  Be sure you look at the URL at bestplaces.net for the various cities to make sure it works as you expect.  For bonus points, create the same 5 x 6 tibble for the same 5 cities using `purrr:map2`!


```{r}
cities <- c("portland", "vancouver", "corvallis", "beaverton", "chicago")
state <- c("oregon", "washington", "oregon", "oregon", "illinois")

big <- map2(state, cities, scrape_bestplaces)
lotsofdata <- list_rbind(big)
lotsofdata
```

```{r}
lotsofdata2 <- bind_rows(scrape_bestplaces("oregon", "portland"), 
                         scrape_bestplaces("washington", "vancouver"), 
                         scrape_bestplaces("oregon", "corvallis"), 
                         scrape_bestplaces("oregon", "beaverton"), 
                         scrape_bestplaces("illinois", "chicago"))
lotsofdata2
```

