---
title: "Iteration"
format:
  html: default
editor_options: 
  chunk_output_type: console
---

You can download this .qmd file from [here](https://github.com/proback/264_fall_2024/blob/main/05_iteration.qmd).  Just hit the Download Raw File button.

This leans on parts of [R4DS Chapter 26: Iteration](https://r4ds.hadley.nz/iteration), in addition to parts of the first edition of R4DS.

```{r}
#| message: false
#| warning: false

# Initial packages required
library(tidyverse)
```


## Iteration

Reducing duplication of code will reduce errors and make debugging much easier.  We've already seen how functions (Ch 25) can help reduce duplication by extracting repeated patterns of code.  Another tool is **iteration**, when you find you're doing the same thing to multiple inputs -- repeating the same operation on different columns or datasets.  

Here we'll see two important iteration paradigms: imperative programming and functional programming.

## Imperation programming for iteration

Examples: for loops and while loops

Pros: relatively easy to learn, make iteration very explicit so it's obvious what's happening, not as inefficient as some people believe

Cons: require lots of bookkeeping code that's duplicated for every loop


Every for loop has three components:

1. output - plan ahead and allocate enough space for output
2. sequence - determines what to loop over; cycles through different values of $i$
3. body - code that does the work; run repeatedly with different values of $i$

```{r}
#| error: TRUE

df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
df

# want median of each column (w/o cutting and pasting)
#   Be careful using square brackets vs double square brackets when
#   selecting elements
median(df[[1]])
median(df[1])

df[1]
df[[1]]

class(df[1])
class(df[[1]])

# basic for loop to take median of each column
output <- vector("double", ncol(df))  # 1. output
for (i in 1:4) {                      # 2. sequence
  output[[i]] <- median(df[[i]])      # 3. body
}
output

# ?seq_along - a safer option if had zero length vectors
output <- vector("double", ncol(df))  # 1. output
for (i in seq_along(df)) {            # 2. sequence
  output[[i]] <- median(df[[i]])      # 3. body
}
output
# use [[.]] even if don't have to to signal working with single elements

# alternative solution - don't hardcode in "4"
output <- vector("double", ncol(df))  # 1. output
for(i in 1:ncol(df)) {                # 2. sequence
  output[i] <- median(df[[i]])        # 3. body
}
output

# another approach - no double square brackets since df not a tibble
df <- as.data.frame(df)
output <- vector("double", ncol(df))  # 1. output
for(i in 1:ncol(df)) {                # 2. sequence
  output[i] <- median(df[,i])         # 3. body
}
output
```

One advantage of `seq_along()`: works with unknown output length.  However, the second approach below is much more efficient, since each iteration doesn't copy all data from previous iterations.

**[Pause to Ponder:]** What does the code below do?  Be prepared to explain both chunks line-by-line!

```{r}
# for loop: unknown output length

means <- c(0, 1, 2)
output <- double()
for (i in seq_along(means)) {
  n <- sample(100, 1)
  output <- c(output, rnorm(n, means[[i]]))
}
str(output)        ## inefficient

out <- vector("list", length(means))
for (i in seq_along(means)) {
  n <- sample(100, 1)
  out[[i]] <- rnorm(n, means[[i]])
}
str(out)           ## more efficient
str(unlist(out))   ## flatten list of vectors into single vector
```

Finally, the `while()` loop can be used with unknown sequence length.  This is used more in simulation than in data analysis.

**[Pause to Ponder:]** What does the following code do?

>> For flip function, sample picks "T" or "H". For our while loop it counts the number of coin flips until it gets 3 "H"s in a row. it flip is heads then we add one to nheads and if else it "resets" back to zero, it will output the number of flips it took to get 3 "H"s in a row. 

```{r}
flip <- function() sample(c("T", "H"), 1)
flips <- 0
nheads <- 0
while (nheads < 3) {
  if (flip() == "H") {
    nheads <- nheads + 1
  } else {
    nheads <- 0
  }
  flips <- flips + 1
}
flips
```


## Using iteration for simulation

[This applet](https://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm?dolphins=1) contains data from a 2005 study on the use of dolphin-facilitated therapy on the treatment of depression.  In that study, 10 of the 15 subjects (67%) assigned to dolphin therapy showed improvement, compared to only 3 of the 15 subjects (20%) assigned to the control group.  But with such small sample sizes, is this significant evidence that the dolphin group had greater improvement of their depressive symptoms?  To answer that question, we can use simulation to conduct a randomization test.

We will simulate behavior in the "null world" where there is no real effect of treatment.  In that case, the 13 total improvers would have improved no matter the treatment assigned, and the 17 total non-improvers would have not improved no matter the treatment assigned.  So in the "null world", treatment is a meaningless label that can be just as easily shuffled among subjects without any effect.  In that world, the fact we observed a 47 percentage point difference in success rates (67 - 20) was just random luck.  But we should ask: how often would we expect a difference as large as 47% by chance, assuming we're living in the null world where there is no effect of treatment?

You could think about simulating this situation with the following steps:

1. write code to calculate the difference in success rates in the observed data

2. write a loop to calculate the differences in success rates from 1000 simulated data sets from the null world.  Store those 1000 simulated differences

3. calculate how often we found a difference in the null world as large as that found in the observed data.  In statistics, when this probability is below .05, we typically reject the null world, and conclude that there is likely a real difference between the two groups (i.e. a "statistically significant" difference)

**[Pause to Ponder:]** Fill in Step 2 in the second R chunk below to carry out the three steps above.  (The first R chunk provides some preliminary code.)  Then describe what you can conclude from this study based on your plot and "p_value" from Step 3.

```{r}
### Preliminary code ###

# generate a tibble with our observed data
dolphin_data <- tibble(treatment = rep(c("Dolphin", "Control"), each = 15),
                       improve = c(rep("Yes", 10), rep("No", 5), 
                                   rep("Yes", 3), rep("No", 12)))
print(dolphin_data, n = Inf)

# `sample()` can be used to shuffle the treatments among the 30 subjects
sample(dolphin_data$treatment)
```

```{r}
#| eval: FALSE

### Fill in Step 2 and remove "eval: FALSE" ###

# Step 1
dolphin_summary <- dolphin_data |>
  group_by(treatment) |>
  summarize(prop_yes = mean(improve == "Yes"))
dolphin_summary
observed_diff <- dolphin_summary[[2]][2] - dolphin_summary[[2]][1]

# Step 2

### Write a loop to create 1000 simulated differences from the null world

# Step 3
null_world <- tibble(simulated_diffs = simulated_diffs)
ggplot(null_world, aes(x = simulated_diffs)) +
  geom_histogram() +
  geom_vline(xintercept = observed_diff, color = "red")

p_value <- sum(abs(simulated_diffs) >= abs(observed_diff)) / 1000
p_value
```


You have written code to conduct a **randomization test for the difference in two proportions**, a powerful test of statistical significance that is demonstrated in the original applet! 


## Functional programming for iteration

Examples: map functions and across()

Pros: less code, fewer errors, code that's easier to read; takes advantage of fact that R is a functional programming language

Cons: little more complicated to master vocabulary and use -- a step up in abstraction


R is a functional programming language. This means that itâ€™s possible to wrap up for loops in a function, and call that function instead of using the for loop directly.  Passing one function to another is a very powerful coding approach!!

```{r}
# Below you can avoid writing separate functions for mean, median, 
#   SD, etc. by column
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)

col_summary <- function(df, fun) {
  out <- vector("double", length(df))
  for (i in seq_along(df)) {
    out[i] <- fun(df[[i]])
  }
  out
}
col_summary(df, median)
col_summary(df, mean)
col_summary(df, IQR)
```


The `purrr` package provides `map` functions to eliminate need for for loops, plus it makes code easier to read!

```{r}
# using map functions for summary stats by column as above
map_dbl(df, mean)
map_dbl(df, median)
map_dbl(df, sd)
map_dbl(df, mean, trim = 0.5)

# map_dbl means make a double vector
# can also do map() for list, map_lgl(), map_int(), and map_chr()

# even more clear
df |> map_dbl(mean)
df |> map_dbl(median)
df |> map_dbl(sd)
```

The across() function from dplyr also works well:

```{r}
df |> summarize(
  n = n(),
  across(.cols = a:d, .fns = median, .names = "median_{.col}")
)

# other ways to repeat across the numeric columns of df:
df |> summarize(
  n = n(),
  across(everything(), median, .names = "median_{.col}")
)

df |> summarize(
  n = n(),
  across(where(is.numeric), median, .names = "median_{.col}")
)

# Here "across" effectively expands to the following code.  Note that 
#   across() will write over old columns unless you change the name!
df |> 
  summarize(
    median_a = median(a),
    median_b = median(b),
    median_c = median(c),
    median_d = median(d),
    n = n()
  )

# And if we're worried about NAs, we can't call median directly, we
#   must create a new function that we can pass options into
df_miss <- df
df_miss[2, 1] <- NA
df_miss[4:5, 2] <- NA
df_miss
df_miss |> 
  summarize(
    across(
      a:d,
      list(
        median = \(x) median(x, na.rm = TRUE),
        n_miss = \(x) sum(is.na(x))
      ),
      .names = "{.fn}_{.col}"
    ),
    n = n(),
  )
# where \ is shorthand for an anonymous function - i.e. you could
#   replace "\" with "function" if you like typing more letters :)

# across-like functions can also be used with filter():

# same as df_miss |> filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))
df_miss |> filter(if_any(a:d, is.na))

# same as df_miss |> filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))
df_miss |> filter(if_all(a:d, is.na))
```

When you input a list of functions (like the lubridate functions below), across() assigns default names as columnname_functionname:

```{r}
library(lubridate)
expand_dates <- function(df) {
  df |> 
    mutate(
      across(where(is.Date), list(year = year, month = month, day = mday))
    )
}

df_date <- tibble(
  name = c("Amy", "Bob"),
  date = ymd(c("2009-08-03", "2010-01-16"))
)

df_date |> 
  expand_dates()
```

Here is default is to summarize all numeric columns, but as with all functions, we can override the default if we choose:

```{r}
summarize_means <- function(df, summary_vars = where(is.numeric)) {
  df |> 
    summarize(
      across({{ summary_vars }}, \(x) mean(x, na.rm = TRUE)),
      n = n(),
      .groups = "drop"
    )
}
diamonds |> 
  group_by(cut) |> 
  summarize_means()

diamonds |> 
  group_by(cut) |> 
  summarize_means(c(carat, x:z))
```


pivot_longer() with group_by() and summarize() also provides a nice solution:

```{r}
long <- df |> 
  pivot_longer(a:d) |> 
  group_by(name) |> 
  summarize(
    median = median(value),
    mean = mean(value)
  )
long

```


Here are a couple of other nice features of `map` functions:
 - perform analyses (like fitting a line) by subgroup
 - extracting components from a model or elements by position

```{r}
# fit linear model to each group based on cylinder
#   - split designed to split into new dfs (unlike group_by)
#   - map returns a vector or list, which can be limiting
map = purrr::map
models <- split(mtcars, mtcars$cyl) |>
  map(function(df) lm(mpg ~ wt, data = df))
models
models[[1]]

# shortcut using purrr - 1-sided formulas
models <- split(mtcars, mtcars$cyl) |> 
  map(~lm(mpg ~ wt, data = .))
models

# extract named components from each model
str(models)
str(models[[1]])
str(summary(models[[1]]))
models |>
  map(summary) |> 
  map_dbl("r.squared")

# can use integer to select elements by position
x <- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))
x |> map_dbl(2)
```


## Iterative techniques for reading multiple files

```{r}
library(readxl)

# our usual path doesn't work with excel files
# read_excel("https://proback.github.io/264_fall_2024/Data/gapminder/1952.xlsx")

# this will work if .xlsx files live in a Data folder that's at the same
#   level as your .qmd file
gap1952 <- read_excel("Data/gapminder/1952.xlsx")
gap1957 <- read_excel("Data/gapminder/1957.xlsx")
```

Since the 1952 and 1957 data have the same 5 columns, if we want to combine this data into a single data set showing time trends, we could simply `bind_rows()` (after adding a 6th column for `year`)

```{r}
gap1952 <- gap1952 |>
  mutate(year = 1952)
gap1957 <- gap1957 |>
  mutate(year = 1957)
gap_data <- bind_rows(gap1952, gap1957)
```

Of course, with 10 more years worth of data still left to read in and merge, this process could get pretty onerous.  Section 26.3 shows how to automate this process in 3 steps:

1. use list.files() to list all the files in a directory

```{r}
paths <- list.files("Data/gapminder", pattern = "[.]xlsx$", full.names = TRUE)
paths
```

2. use purrr::map() to read each of them into a list (we will discuss lists more in 06_data_types.qmd)

```{r}
gap_files <- map(paths, readxl::read_excel)
length(gap_files)
str(gap_files)
gap_files[[1]]   # pull off the first object in the list (i.e. 1952 data)
```

3. use purrr::list_rbind() to combine them into a single data frame

```{r}
gap_tidy <- list_rbind(gap_files)
class(gap_tidy)
gap_tidy
```

We could even do all steps in a single pipeline:

```{r}
list.files("Data/gapminder", pattern = "[.]xlsx$", full.names = TRUE) |>
  map(readxl::read_excel) |>
  list_rbind()
```

Note that we are lacking a 6th column with the year represented by each row of data.  Here is one way to solve that issue:

```{r}
# This extracts file names, which are carried along as data frame names by
#   map functions
paths |> set_names(basename)

# The middle line ensures that each of the 12 data frames in the list for
#   gap_files has a name determined by its filepath, unlike the gap_files
#   we created in step 2 above, which had no names (we could only identify
#   data frames by their position)
gap_files <- paths |> 
  set_names(basename) |>  
  map(readxl::read_excel)

# Now we can extract a particular year by its name:
gap_files[["1962.xlsx"]]

# Finally, take advantage of the `names_to` argument in list_rbind to 
#   create that 6th column with `year`
gap_tidy <- paths |> 
  set_names(basename) |> 
  map(readxl::read_excel) |> 
  list_rbind(names_to = "year") |> 
  mutate(year = parse_number(year))
```

You could then save your result using `write_csv` so you don't have to run the reading and wrangling code every time!


## On Your Own

1. Compute the mean of every column of the `mtcars` data set using (a) a for loop, (b) a `map` function, (c) the across() function, and (d) pivot_longer().

```{r}

# a) basic for loop to take mean of each column
output <- vector("double", ncol(mtcars))  # 1. output
for (i in 1:seq_along(mtcars)) {                      # 2. sequence
  output[[i]] <- mean(mtcars[[i]])      # 3. body
}
output

# b) map function
map_dbl(mtcars, mean)

# c) across function 
mtcars |> summarize(
  n = n(),
  across(everything(), mean, .names = "mean_{.col}")
)

# d) pivot longer
mtcars |>
  pivot_longer(1:seq_along(mtcars)) |>
  group_by(name) |>
  summarize(
    mean = mean(value)
  )

```


2. Write a function that prints the mean of each *numeric* column in a data frame.  Try it on the `iris` data set. (Hint: `keep(is.numeric)`)


3. Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:

```{r}
out <- ""
for (x in letters) {
  out <- stringr::str_c(out, x)
}
out


x <- runif(100)
out <- vector("numeric", length(x))
out[1] <- x[1]
for (i in 2:length(x)) {
  out[i] <- out[i - 1] + x[i]
}
out

```


4. Compute the number of unique values in each column of the `iris` data set using at least 2 of your favorite iteration methods.  Bonus points if you can use pivot_longer()!

```{r}
# pivot longer
iris |>
  pivot_longer(everything(), values_transform = as.character) |>
  group_by(name) |>
  summarize(distinct = n_distinct(value))

# map
map_dbl(iris, n_distinct)

# for loop
output <- vector("double", ncol(iris))  # 1. output
for (i in seq_along(iris)) {                      # 2. sequence
  output[[i]] <- n_distinct(iris[[i]])      # 3. body
}
output

# across
iris |>
  summarize(across(everything(), n_distinct, "{.cols}"))
```


5. Carefully explain each step in the pipeline below:

```{r}
show_missing <- function(df, group_vars, summary_vars = everything()) {
  df |> 
    group_by(pick({{ group_vars }})) |> #this 
    summarize(
      across({{ summary_vars }}, \(x) sum(is.na(x))),
      .groups = "drop"
    ) |>
# for column x, only selecting columns where there is at least one NA
    select(where(\(x) any(x > 0))) 
}
nycflights13::flights |> show_missing(c(year, month, day))
```


6. Write a function called `summary_stats()` that allows a user to input a tibble, numeric variables in that tibble, and summary statistics that they would like to see for each variable.  Using `across()`, the function's output should look like the example below.

```{r}
#| eval: FALSE

summary_stats(mtcars, 
              vars = c(mpg, hp, wt), 
              stat_fcts = list(mean = mean, 
                               median = median, 
                               sd = sd, 
                               IQR = IQR))

#  mpg_mean mpg_median   mpg_sd mpg_IQR  hp_mean hp_median    hp_sd hp_IQR
#1 20.09062       19.2 6.026948   7.375 146.6875       123 68.56287   83.5
#  wt_mean wt_median     wt_sd  wt_IQR  n
#1 3.21725     3.325 0.9784574 1.02875 32

```


7. The **power** of a statistical test is the probability that it rejects the null hypothesis when the null hypothesis is false.  In other words, it's the probability that a statistical test can detect when a true difference exists.  The power depends on a number of factors, including:

- sample size
- type I error level (probability of declaring there is a statistically significant difference when there really isn't)
- variability in the data
- size of the true difference

The following steps can be followed to simulate a power calculation using iteration techniques:

a. generate simulated data where is a true difference or effect

b. run your desired test on the simulated data and record if the null hypothesis was rejected or not (i.e. if the p-value was below .05)

c. repeat (a)-(b) a large number of times and record the total proportion of times that the null hypothesis was rejected; that proportion is the power of your test under those conditions


Create a power curve for a two-sample t-test by filling in Step C below and then removing `eval: FALSE`:

```{r}
#| eval: FALSE

# Step A

# set parameters for two-sample t-test
mean1 <- 100   # mean response in Group 1
truediff <- 5    # true mean difference between Groups 1 and 2
mean2 <- mean1 + truediff   # mean response in Group 2
sd1 <- 10   # standard deviation in Group 1
sd2 <- 10   # standard deviation in Group 2
n1 <- 20    # sample size in Group 1
n2 <- 20    # sample size in Group 2
numsims <- 1000   # number of simulations (iterations) to run

# generate sample data for Groups 1 and 2 based on normal distributions
#   with the parameters above (note that there is truly a difference in means!)
samp1 <- rnorm(n1, mean1, sd1)
samp2 <- rnorm(n2, mean2, sd2)

# organize the simulated data into a tibble
sim_data <- tibble(response = c(samp1, samp2), 
       group = c(rep("Group 1", n1), rep("Group 2", n2)))
sim_data

# Step B

# exploratory analysis of the simulated data
mosaic::favstats(response ~ group, data = sim_data)
ggplot(sim_data, aes(x = response, y = group)) +
  geom_boxplot()

# run a two-sample t-test to see if there is a significant difference
#   in means between Groups 1 and 2 (i.e. is the p-value < .05?)
p_value <- t.test(x = samp1, y = samp2)$p.value
p_value
p_value < .05   # if TRUE, then we reject the null hypothesis and conclude
                #   there is a statistically significant difference

# Step C

# find the power = proportion of time null is rejected when
#   true difference is not 0 (i.e. number of simulated data sets that
#   result in p-values below .05)

```
