---
title: "Text analysis"
format:
  html: default
editor_options: 
  chunk_output_type: console
---

You can download this .qmd file from [here](https://github.com/proback/264_fall_2024/blob/main/13_text_analysis.qmd).  Just hit the Download Raw File button.

We will build on techniques you learned in SDS 164 using parts of [Text Mining with R](https://www.tidytextmining.com/) by Silge and Robinson.


```{r}
#| include: FALSE
library(tidyverse)
library(tidytext)
library(textdata)
library(wordcloud)
library(wordcloud2)
library(viridis)
library(ggthemes)
library(gutenbergr)
library(rvest)
library(widyr)
library(tm)
library(topicmodels)
```

## Text analysis of books from Project Gutenberg

We will use the `gutenbergr` package to obtain several works from [Project Gutenberg](www.gutenberg.org) to examine using text analysis tools.

```{r}
# How I obtained the three works from Project Gutenberg

# Notes:
# - might have to find mirror at https://www.gutenberg.org/MIRRORS.ALL
# - 84 = Frankenstein; 345 = Dracula; 43 = Jekyll and Hyde

# three_works <- gutenberg_download(
#  c(84, 345, 43),
#  meta_fields = "title",
#  mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")

# write_csv(three_works, "~/264_fall_2024/Data/three_works.csv")
```

```{r}
# three_works <- read_csv("https://proback.github.io/264_fall_2024/Data/three_works.csv")
# three_works2 <- read_csv("Data/three_works.csv") 

library(RCurl)
#three_works <- read_csv(
  #file = getURL("https://raw.githubusercontent.com/proback/264_fall_2024/refs/heads/main/Data/three_works.csv", .encoding = "UTF-8"))

three_works <- read_csv("Class Files/00_Data/three_works.csv")
three_works |> count(title)
three_works

frankenstein <- three_works |>
  filter(str_detect(title, "Frankenstein"))
```

We will begin by looking at a single book (Frankenstein) and then we'll compare and contrast 3 books (Frankenstein, Dracula, and Jekyll and Hyde).


# 1. Tidy Text Mining!

Now it's time to tokenize and tidy this text data.

```{r}
tidy_book <- frankenstein |>
  mutate(line = row_number()) |>
  unnest_tokens(word, text, token = "words")   # (new name, input)
# default for unnest_tokens is token = "words", but can also use
#   "characters", "ngrams" with say n=2, "sentences", "regex" with
#   pattern, "tweets" with strip_url, etc.

tidy_book   # one row per word, instead of one per line

frankenstein |> slice_tail(n = 10)
tidy_book |> slice_tail(n = 20)
```

What are the most common words?

```{r}
tidy_book |>
  count(word, sort = TRUE)
```


## Stop words (get rid of common but not useful words)

Note: If you get "Error in loadNamespace(name) : there is no package called ‘stopwords’", first install package `stopwords`.

```{r}
get_stopwords() |> print(n = 50)   # snowball is default - somewhat smaller
get_stopwords(source = "smart") |> print(n = 50)   

# will sometimes want to store if using over and over
#   - later with shiny apps will have to store and write as data file
smart_stopwords <- get_stopwords(source = "smart")
```

Try out using different languages (`language`) and different lexicons (`source`).

## Another try at most common words

```{r}
tidy_book |>
  anti_join(smart_stopwords) |>
  count(word, sort = TRUE) |>
  filter(word != "NA") |>
  slice_max(n, n = 20) |>
  ggplot(aes(fct_reorder(word, n), n)) +
  geom_col() +
  coord_flip()
```


## Sentiment analysis

Explore some sentiment lexicons.  You'll want to match your choice of sentiment lexicon to your purpose:

- afinn: scored from -5 (very negative) to +5 (very positive)
- nrc: words are labeled with emotions like anger, fear, sadness, etc.  There can be more than one row per word.
- bing: binary - listed words are either negative or positive

```{r}
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "nrc")
get_sentiments(lexicon = "bing")

bing_sentiments <- get_sentiments(lexicon = "bing")
```

Implement sentiment analysis using an `inner_join()`, so you only consider words both in your text and in the lexicon.

```{r}
tidy_book |>   
  inner_join(bing_sentiments) |>
  count(sentiment)
```

What words contribute the most to sentiment scores for Frankenstein?  Let's walk through this pipe step-by-step.

```{r}
tidy_book |>
  inner_join(bing_sentiments) |>
  count(sentiment, word, sort = TRUE) |>
  group_by(sentiment) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
    geom_col() +  
    coord_flip() +
    facet_wrap(~ sentiment, scales = "free")
```

```{r}
# Check out which words are associated with specific nrc emotions
get_sentiments("nrc") |>
  count(sentiment)

get_sentiments("nrc") |> 
  filter(sentiment == "joy") |>
  inner_join(tidy_book) |>
  count(word, sort = TRUE)

get_sentiments("nrc") |> 
  filter(sentiment == "anger") |>
  inner_join(tidy_book) |>
  count(word, sort = TRUE)
```

Make a wordcloud for Frankenstein.

```{r}
#| warning: FALSE
#| message: FALSE

# wordcloud wants a column with words and another column with counts
words <- tidy_book |>
  anti_join(stop_words) |>
  count(word) |>
  filter(word != "NA") |>
  arrange(desc(n))

# Note: this will look better in html than in the Plots window in RStudio
wordcloud(
  words = words$word, 
  freq = words$n, 
  max.words = 100, 
  random.order = FALSE
)

# See Z's R Tip of the Day for suggestions on options
wordcloud(
  words = words$word, 
  freq = words$n, 
  max.words = 200, 
  random.order = FALSE, 
  rot.per = 0.35,
  scale = c(3.5, 0.25),
  colors = brewer.pal(6, "Dark2"))

# Or for even cooler looks, use wordcloud2 (for html documents)
words_df <- words |>
  slice_head(n = 80) |>
  data.frame()

wordcloud2(
  words_df, 
  size = .25, 
  shape = 'star',
  minSize = 10
)

# A couple of helpful links for customizing wordclouds:
#   https://www.youtube.com/watch?v=0cToDzeDLRI
#   https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a
```

You could do cool stuff here, like color the words by sentiment! 


# 2. What is a document about?

Above, we read in a corpus called `three_works`. We'll use that here!

Count the word frequencies by title in this collection.

```{r}
book_words <- three_works |>
  group_by(title) |>
  mutate(linenumber = row_number()) |>
  ungroup() |>
  unnest_tokens(word, text) 
  
book_word_count <- book_words |>
  count(word, title, sort = TRUE)

book_word_count
```

Look at positive/negative sentiment trajectory over the novels

```{r}
book_words |>
  inner_join(bing_sentiments) |>
  count(title, index = linenumber %/% 80, sentiment) |>
# index approximates a chapter (every 80 lines)
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  mutate(sentiment = positive - negative) |>
  ggplot(aes(x = index, y = sentiment, fill = title)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~title, ncol = 2, scales = "free_x")
```


## Calculate tf-idf.

The tf-idf statistic is term frequency times inverse document frequency, a quantity used for identifying terms that are especially important to a particular document.  The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.  We want to find words that define one document as opposed to others.

- tf = term frequency = proportion of times a term appears in a document.  
- idf = inverse document frequency = log(number of documents / number of documents with the term), so that terms that appear in fewer documents are weighted higher, since those rarer words provide more information.  

There's really no theory behind multiplying the two together - it just tends to work in practice.  See [this wikipedia entry](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for more details.  (See also [this site](https://towardsdatascience.com/whats-in-a-word-da7373a8ccb) for a nice description of weaknesses of tf-idf.)

```{r}
book_tfidf <- book_word_count |>
  bind_tf_idf(word, title, n)

book_tfidf   # note idf = 0 when it appears in every document
```

Find *high* tf-idf words.  The highest words will appear relatively often in one document, but not at all in others.

```{r}
book_tfidf |>
  arrange(-tf_idf)
```

How can we visualize this? Let's go step-by-step.

```{r}
book_tfidf |>
  group_by(title) |>
  arrange(desc(tf_idf)) |>
  slice_max(tf_idf, n = 10) |>
  ungroup() |>
  ggplot(aes(x = fct_reorder(word, tf_idf), y = tf_idf, fill = title)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    facet_wrap(~title, scales = "free")
# kind of boring - mostly proper nouns
```


## N-grams... and beyond!

Let's return to Frankenstein and look at 2-word combinations:

```{r}
tidy_ngram <- frankenstein |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  filter(bigram != "NA")

tidy_ngram
```

What are the most common bigrams?

```{r}
tidy_ngram |>
  count(bigram, sort = TRUE)
```

Let's use `separate()` from tidyr to remove stop words.

```{r}
# stop_words contains 1149 words from 3 lexicons
bigrams_filtered <- tidy_ngram |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE)
bigrams_filtered
```

Now extend from a single document to our collection of documents.  See which two-word combinations best identify books in the collection.

```{r}
book_twowords <- three_works |>
  group_by(title) |>
  mutate(linenumber = row_number()) |>
  ungroup() |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  filter(bigram != "NA")
 
book_twowords |>
  count(bigram, sort = TRUE)

bigrams_filtered <- book_twowords |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE) |>
  filter(!is.na(word1) & !is.na(word2))

bigrams_filtered 

bigrams_united <- bigrams_filtered |>
  unite(bigram, word1, word2, sep = " ")

bigrams_united 

bigram_tf_idf <- book_twowords |>
  count(title, bigram) |>
  bind_tf_idf(bigram, title, n) |>
  arrange(desc(tf_idf)) 

bigram_tf_idf |> arrange(desc(tf_idf))

bigram_tf_idf |>
  group_by(title) |>
  arrange(desc(tf_idf)) |>
  slice_max(tf_idf, n = 10) |>
  ungroup() |>
  ggplot(aes(x = fct_reorder(bigram, tf_idf), y = tf_idf, fill = title)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    facet_wrap(~title, scales = "free")

```

## Sentence context using bigrams

Bigrams can also help us dive deeper into sentiment analysis.  For example, even though "happy" carries positive sentiment, but when preceded by "not" as in this sentence: "I am not happy with you!" it conveys negative sentiment.  Context can matter as much as mere presence!  

Let's see which words associated with an afinn sentiment are most frequently preceded by "not":

```{r}
afinn <- get_sentiments("afinn")

bigrams_separated <- book_twowords |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  count(word1, word2, sort = TRUE) |>
  filter(!is.na(word1) & !is.na(word2))

bigrams_separated |> filter(word1 == "not")

not_words <- bigrams_separated |>
  filter(word1 == "not") |>
  inner_join(afinn, by = c(word2 = "word")) |>
  arrange(desc(n))

not_words
```

We could then ask which words contributed the most in the “wrong” direction. One approach is to multiply their value by the number of times they appear (so that a word with a value of +3 occurring 10 times has as much impact as a word with a sentiment value of +1 occurring 30 times).

```{r}
not_words |>
  mutate(contribution = n * value) |>
  arrange(desc(abs(contribution))) |>
  head(20) |>
  mutate(word2 = reorder(word2, contribution)) |>
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")
```

With this approach, we could expand our list of negation words, and then possibly even adjust afinn totals to reflect context!

```{r}
# An example of expanding the list of negation words
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated |>
  filter(word1 %in% negation_words) |>
  inner_join(afinn, by = c(word2 = "word")) |>
  arrange(desc(n))

negated_words

negated_words |>
  mutate(contribution = n * value) |>
  arrange(desc(abs(contribution))) |>
  group_by(word1) |>
  slice_max(abs(contribution), n = 10) |>
  ungroup() |>
  mutate(word2 = reorder(word2, contribution)) |>
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ word1, scales = "free") +
    labs(x = "Sentiment value * number of occurrences",
         y = "Words preceded by negation term")
```

## Creating a network graph

If we are interested in visualizing all relationships among words or bigrams, we can arrange the words into a network, which is a combination of connected nodes.  A network graph has three elements:

- from: the node an edge is coming from
- to: the node an edge is going towards
- weight: A numeric value associated with each edge

The `igraph` package has many powerful functions for manipulating and analyzing networks. One way to create an igraph object from tidy data is the `graph_from_data_frame()` function.  Let's see how it works using Frankenstein:

```{r}
#| warning: FALSE
#| message: FALSE

library(igraph)

# filter for only relatively common combinations
bigram_graph <- bigrams_filtered |>
  filter(n > 10) |>
  graph_from_data_frame()

bigram_graph

# Use ggraph to convert into a network plot
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

# polish the graph
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

## Correlating pairs of words

Tokenizing by n-gram is a useful way to explore pairs of adjacent words. However, we may also be interested in words that tend to co-occur within particular documents or particular chapters, even if they don’t occur next to each other.  Following Section 4.2 of Text Mining with R, we will use the `widyr` package.

Consider the book “Frankenstein” divided into 10-line sections. We may be interested in what words tend to appear within the same section.

```{r}
frankenstein_section_words <- frankenstein |>
  select(-gutenberg_id) |>
  mutate(section = row_number() %/% 10) |> 
  filter(section > 0) |>
  unnest_tokens(word, text) |> 
  filter(!word %in% stop_words$word,
         !is.na(word))

frankenstein_section_words 

# count words co-occuring within sections
library(widyr)
word_pairs <- frankenstein_section_words |>
  pairwise_count(word, section, sort = TRUE)

word_pairs

# What words occur most often with "life"?
word_pairs |>
  filter(item1 == "life")
```

We can quantify pairwise correlation using the Phi coefficient (which simplifies to the Pearson correlation coefficient with numeric data).  The Phi coefficient measures how often two words appear together relative to how often they appear separately (so we don't just pick up the most common words).

```{r}
# we need to filter for at least relatively common words first
word_cors <- frankenstein_section_words |>
  group_by(word) |>
  filter(n() >= 10) |>
  pairwise_cor(word, section, sort = TRUE)

word_cors

# What words are most correlated with "life"?
word_cors |>
  filter(item1 == "life")
```

Plot words most associated with a set of interesting words:

```{r}
word_cors |>
  filter(item1 %in% c("life", "death", "father", "eyes")) |>
  group_by(item1) |>
  slice_max(correlation, n = 6) |>
  ungroup() |>
  mutate(item2 = reorder(item2, correlation)) |>
  ggplot(aes(item2, correlation)) +
    geom_bar(stat = "identity") +
    facet_wrap(~ item1, scales = "free") +
    coord_flip()
```

Finally, create a network graph to visualize the correlations and clusters of words that were found by the widyr package

```{r}
#| warning: FALSE
#| message: FALSE

set.seed(2016)

word_cors |>
  filter(correlation > .25) |>
  graph_from_data_frame() |>
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
```

## Topic Modeling

As described in Ch 6 of Text Mining with R:

> In text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.

> Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.

We will attempt to apply LDA to our collection of three works.  While not a typical application of topic modeling, it'll be interesting to see if any common themes or groupings emerge.

Again, from Ch 6:

> Latent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

>> Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”

>> Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

> LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. 

In order to implement LDA on our three books, we need to first "cast" our tidy data as a document-term matrix (DTM) where:

- each row represents one document (such as a book or article),
- each column represents one term, and
- each value (typically) contains the number of appearances of that term in that document.

From Section 5.2 of Text Mining with R:

> Since most pairings of document and term do not occur (they have the value zero), DTMs are usually implemented as sparse matrices. These objects can be treated as though they were matrices (for example, accessing particular rows and columns), but are stored in a more efficient format. 

> DTM objects cannot be used directly with tidy tools, just as tidy data frames cannot be used as input for most text mining packages. Thus, the tidytext package provides two verbs (`tidy` and `cast`) that convert between the two formats.

> A DTM is typically comparable to a tidy data frame after a count or a group_by/summarize that contains counts or another statistic for each combination of a term and document.

```{r}
# cast the collection of 3 works as a document-term matrix
library(tm)
three_books_dtm <- book_word_count |>
  filter(!word %in% stop_words$word,
         !is.na(word)) |>
  cast_dtm(title, word, n)

# set a seed so that the output of the model is predictable
library(topicmodels)
three_books_lda <- LDA(three_books_dtm, k = 2, control = list(seed = 1234))
three_books_lda
```

After fitting our LDA model, we will first focus on the beta variable, which is the probability of a word being generated by a specific topic.  Then we'll turn to the gamma variable, which are the per-document per-topic probabilities, or the proportion of words from a document generated by a specific topic.

```{r}
three_books_topics <- tidy(three_books_lda, matrix = "beta")
three_books_topics

# Find the most common words within each topic
three_books_top_terms <- three_books_topics |>
  group_by(topic) |>
  slice_max(beta, n = 10) |> 
  ungroup() |>
  arrange(topic, -beta)

three_books_top_terms |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()
# This would be much cooler with more documents and if we were able
#   to anti_join to remove proper nouns

# Find words with greatest difference between two topics, using log ratio
beta_wide <- three_books_topics |>
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |> 
  filter(topic1 > .001 | topic2 > .001) |>
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide

beta_wide |>
  arrange(desc(abs(log_ratio))) |>
  slice_max(abs(log_ratio), n = 20) |>
  mutate(term = reorder(term, log_ratio)) |>
  ggplot(aes(log_ratio, term, fill = log_ratio > 0)) +
    geom_col(show.legend = FALSE) +
    labs(x = "Log ratio of Beta values",
         y = "Words in three works")


# find the gamma variable for each document and topic
three_books_documents <- tidy(three_books_lda, matrix = "gamma")
three_books_documents
# Dracula = Topic 2; other two books = Topic 1!
```




# On Your Own: Harry Potter

The `potter_untidy` dataset includes the text of 7 books of the Harry Potter series by J.K. Rowling. For a brief overview of the books (or movies), see this quote from Wikipedia: 

> Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's conflict with Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles (non-magical people).

```{r}
#| include: FALSE

# potter_untidy <- read_csv("https://proback.github.io/264_fall_2024/Data/potter_untidy.csv") |>
# potter_untidy <- read_csv("Data/potter_untidy.csv")

potter_untidy <- read_csv(
  file = getURL("https://raw.githubusercontent.com/proback/264_fall_2024/refs/heads/main/Data/potter_untidy.csv", .encoding = "UTF-8")) |>
  mutate(title = fct_reorder(title, book_num))

potter_untidy

potter_tidy <- potter_untidy |>
  unnest_tokens(output = word, input = text)

potter_tidy

# potter_locations <- read_csv("https://proback.github.io/264_fall_2024/Data/potterlocations.csv") |>
# potter_locations <- read_csv("Data/potterlocations.csv")

potter_locations <- read_csv(
  file = getURL("https://raw.githubusercontent.com/proback/264_fall_2024/refs/heads/main/Data/potterlocations.csv", .encoding = "UTF-8")) |>
  mutate(value = str_to_lower(value))

potter_locations

# potter_names <- read_csv("https://proback.github.io/264_fall_2024/Data/potternames.csv") |>
# potter_names <- read_csv("Data/potternames.csv") |>
potter_names <- read_csv(
  file = getURL("https://raw.githubusercontent.com/proback/264_fall_2024/refs/heads/main/Data/potternames.csv", .encoding = "UTF-8")) |>
  mutate(fullname = str_to_lower(fullname),
         firstname = str_to_lower(firstname),
         lastname = str_to_lower(lastname))

potter_names

# potter_spells <- read_csv("https://proback.github.io/264_fall_2024/Data/potter_spells.csv") |>
# potter_spells <- read_csv("Data/potter_spells.csv") |>
potter_spells <- read_csv(
  file = getURL("https://raw.githubusercontent.com/proback/264_fall_2024/refs/heads/main/Data/potter_spells.csv", .encoding = "UTF-8")) |>
  filter(spell_name != "Point Me")

potter_spells
```


## A few analyses from SDS 164:

```{r}
# 10 most common words in each book, excluding stop words
potter_tidy |>
  count(title, word) |>
  anti_join(stop_words) |>
  group_by(title) |>
  slice_max(n, n = 10) |>
  mutate(rank = 1:10) |>
   select(-n) |>
  pivot_wider (names_from = title, values_from = word) |>
  print(width = Inf)

# Repeat above after removing character first and last names
potter_tidy |>
  count(title, word) |>
  anti_join(stop_words) |>
  anti_join(potter_names, join_by(word == firstname)) |> 
  anti_join(potter_names, join_by(word == lastname)) |>
  group_by(title) |>
  slice_max(n, n = 10, with_ties = FALSE) |>
  mutate(rank = 1:10) |>
   select(-n) |>
  pivot_wider (names_from = title, values_from = word) |>
  print(width = Inf)
# still get "harry's" and "professor" but otherwise looks good

# top 10 names in each book (after excluding "the")
potter_tidy |>
  count(title, word) |>
  semi_join(potter_names, join_by(word == firstname)) |>
  filter(word != "the") |> # ADD for #6
  group_by(title) |>
  slice_max(n, n = 10, with_ties = FALSE) |>
  mutate(rank = 1:10) |>
   select(-n) |>
  pivot_wider (names_from = title, values_from = word) |>
  print(width = Inf)

# spell statistics by book
potter_tidy |>
  left_join(potter_spells, join_by(word == first_word)) |>
  group_by(title) |>
  summarize(num_spells_cast = sum(!is.na(spell_name)), 
            spells_per_10kwords = mean(!is.na(spell_name)) * 10000,
            num_unique_spells = n_distinct(spell_name) - 1)  # Why -1??

# plot of top spells by book
potter_tidy |>
  left_join(potter_spells, join_by(word == first_word)) |>
  drop_na(spell_name) |>  
  mutate(spell_name = fct_infreq(spell_name),
         spell_name = fct_lump_n(spell_name, n = 5)) |>
    count(title, spell_name) |>
  ggplot() +
  geom_col(aes(x = title, y = n, fill = spell_name), position = "stack")  

```


## New stuff!

1. What words contribute the most to negative and positive sentiment scores?  Show a faceted bar plot of the top 10 negative and the top 10 positive words (according to the "bing" lexicon) across the entire series.

```{r}
bing_sentiments <- get_sentiments(lexicon = "bing")
# potter_tidy |>
#   inner_join(bing_sentiments, relationship = "many-to-many") |>
#   count(sentiment, word, sort = TRUE) |>
#   group_by(sentiment) |>
#   slice_max(n, n=10) |>
#   ungroup() |>
#   ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
#   geom_col() +
#   coord_flip() +
#   facet_wrap(~ sentiment, scales = "free")

potter_tidy |>
  inner_join(bing_sentiments, relationship = "many-to-many") |>
  count(sentiment, word, sort = TRUE) |>
  group_by(sentiment) |>
  slice_max(n, n=10) |>
  ungroup() |>
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip()
```

2. Find a list of the top 10 words associated with "fear" and with "trust" (according to the "nrc" lexicon) across the entire series.

```{r}
nrc_sentiments <- get_sentiments(lexicon = "nrc")

nrc_sentiments |>
  filter(sentiment == "fear" | sentiment == "trust") |>
  inner_join(potter_tidy, relationship = "many-to-many") |>
  count(sentiment, word, sort = TRUE) |>
  group_by(sentiment) |>
  slice_max(n, n=10) |>
  ungroup() |>
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip()
```


3. Make a wordcloud for the entire series after removing stop words using the "smart" source.

```{r}
smart_stopwords <- get_stopwords(source = "smart")

potter_words <- potter_tidy |>
  anti_join(smart_stopwords) |>
  anti_join(potter_names, join_by(word == firstname)) |>
  anti_join(potter_names, join_by(word == lastname)) |>
  count(word) |>
  arrange(desc(n))

set.seed(1234)
wordcloud(
    words = potter_words$word,
    freq = potter_words$n,
    scale=c(3,0.3),
    max.words = 100, 
    random.order = FALSE, 
    rot.per = 0,            
    colors = brewer.pal(4, "Dark2"))
```


4. Create a wordcloud with the top 20 negative words and the top 20 positive words in the Harry Potter series according to the bing lexicon.  The words should be sized by their respective counts and colored based on whether their sentiment is positive or negative.  (Feel free to be resourceful and creative to color words by a third variable!)

```{r}
sent_potter <- bing_sentiments |>
  inner_join(potter_tidy, relationship = "many-to-many") |>
  count(sentiment, word, sort = TRUE) |>
  group_by(sentiment) |>
  slice_max(n, n=20) |>
  mutate(
    sentiment = ifelse(sentiment == "positive", "green", "red")
  )

wordcloud(
    words = sent_potter$word,
    freq = sent_potter$n,
    random.order = FALSE,
    max.words = 100,
    ordered.colors = TRUE,
    color = sent_potter$sentiment)

# sent_potter <- potter_tidy |>
#   inner_join(bing_sentiments, relationship = "many-to-many") |>
#   count(sentiment, word, sort = TRUE) |>
#   group_by(sentiment) |>
#   slice_max(n, n=20)
# 
# set.seed(1234)
# wordcloud(
#     words = sent_potter$word,
#     freq = sent_potter$n,
#     random.order = FALSE,
#     ordered.colors = TRUE,
#     color = brewer.pal(4, "Dark2")[factor(sent_potter$sentiment)])

```


5. Make a faceted bar chart to compare the positive/negative sentiment trajectory over the 7 Harry Potter books.  You should have one bar per chapter (thus chapter becomes the index), and the bar should extend up from 0 if there are more positive than negative words in a chapter (according to the "bing" lexicon), and it will extend down from 0 if there are more negative than positive words.

```{r}
bing_sentiments <- get_sentiments(lexicon = "bing")

bing_sentiments |>
  inner_join(potter_tidy, relationship = "many-to-many") |>
  count(title, chapter, sentiment, sort = TRUE) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  mutate(sentiment = positive - negative) |>
  ggplot(aes(x = chapter, y = sentiment, fill = title)) +
  geom_col(show.legend = FALSE) +
    facet_wrap(~title, ncol = 2, scales = "free_x")
```


6. Repeat (5) using a faceted scatterplot to show the average sentiment score according to the "afinn" lexicon for each chapter.  (Hint: use `mutate(chapter_factor = factor(chapter))` to treat chapter as a factor variable.)

```{r}
afinn_sentiments <- get_sentiments("afinn")

afinn_sentiments |>
  inner_join(potter_tidy, relationship = "many-to-many") |>
  group_by(title, chapter) |>
  summarize(avg_sentiment = mean(value)) |>
  mutate(chapter_factor = factor(chapter)) |>
  ggplot(aes(x = chapter_factor, y = avg_sentiment, color = title, group = title)) +
  geom_line() +
  geom_point() + 
    facet_wrap(~title, ncol = 2, scales = "free_x")
```

7. Make a faceted bar plot showing the top 10 words that distinguish each book according to the tf-idf statistic.

```{r}
potter_tfidf <- potter_tidy |>
  group_by(title) |>
  count(word) |>
  bind_tf_idf(word, title, n) |>
  arrange(-tf_idf)
```


```{r}
potter_tfidf |>
  group_by(title) |>
  slice_max(tf_idf, n = 10) |>
  ungroup() |>
  ggplot(aes(x = fct_reorder(word, tf_idf), y = tf_idf, fill = title)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    facet_wrap(~title, scales = "free")
```

8. Repeat (7) to show the top 10 2-word combinations that distinguish each book. 

```{r}
potter_bigram <- potter_untidy |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  filter(bigram != "NA")

potter_bigram_filtered <- potter_bigram |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE) |>
  unite(bigram, word1, word2, sep = " ")

potter_bigram_filtered

potter_goodgrams <- potter_bigram_filtered |>
  left_join(potter_bigram) |>
  count(title, bigram) |>
  bind_tf_idf(bigram, title, n) |>
  arrange(desc(tf_idf)) 

potter_goodgrams |>
  group_by(title) |>
  arrange(desc(tf_idf)) |>
  slice_max(tf_idf, n = 10) |>
  ggplot(aes(x = fct_reorder(bigram, tf_idf), y = tf_idf, fill = title)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    facet_wrap(~title, scales = "free")
```


9. Find which words contributed most in the "wrong" direction using the afinn sentiment combined with how often a word appears among all 7 books.  Come up with a list of 4 negation words, and for each negation word, illustrate the words associated with the largest "wrong" contributions in a faceted bar plot.


```{r}
# An example of expanding the list of negation words
negation_words <- c("not", "no", "never", "without")

potter_bigram_separated <- potter_bigram |> 
  separate(bigram, c("word1", "word2"), sep = " ") |>
  count(word1, word2, sort = TRUE) |>
  filter(!is.na(word1) & !is.na(word2)) 

negated_words <- potter_bigram_separated |>
  filter(word1 %in% negation_words) |>
  inner_join(afinn_sentiments, by = c(word2 = "word")) |>
  arrange(desc(n))

negated_words |>
  mutate(contribution = n * value) |>
  arrange(desc(abs(contribution))) |>
  group_by(word1) |>
  slice_max(abs(contribution), n = 10) |>
  ungroup() |>
  mutate(word2 = reorder(word2, contribution)) |>
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ word1, scales = "free") +
    labs(x = "Sentiment value * number of occurrences",
         y = "Words preceded by negation term")
```


10. Select a set of 4 "interesting" terms and then use the Phi coefficient to find and plot the 6 words most correlated with each of your "interesting" words.  Start by dividing `potter_tidy` into 80-word sections and then remove names and spells and stop words.

```{r}
potter_section <- potter_tidy |>
  mutate(section = row_number() %/% 80) |>
  filter(!word %in% stop_words$word,
         !word %in% potter_names$firstname,
         !word %in% potter_names$lastname,
         !word %in% potter_spells$first_word,
         !word %in% potter_spells$second_word,
         !is.na(word))

potter_section |>
  pairwise_count(word, section, sort = TRUE)

potter_cors <- potter_section |>
  group_by(word) |>
  filter(n() >= 10) |>
  pairwise_cor(word, section, sort = TRUE)

potter_cors |>
  filter(item1 %in% c("castle", "train", "map", "owl")) |>
  group_by(item1) |>
  slice_max(correlation, n = 6) |>
  ungroup() |>
  mutate(item2 = reorder(item2, correlation)) |>
  ggplot(aes(item2, correlation)) +
    geom_bar(stat = "identity") +
    facet_wrap(~ item1, scales = "free") +
    coord_flip()
```


11. Create a network graph to visualize the correlations and clusters of words that were found by the `widyr` package in (10).

```{r}
# for a correlation over .45
potter_cors |>
  filter(correlation > .45) |>
  graph_from_data_frame() |>
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
```


12. Use LDA to fit a 2-topic model to all 7 Harry Potter books.  Be sure to remove names, spells, and stop words before running your topic models.  (a) Make a plot to illustrate words with greatest difference between two topics, using log ratio.  (b) Print a table with the gamma variable for each document and topic.  Based on (a) and (b), can you interpret what the two topics represent?

```{r}
potter_dtm <- potter_tidy |>
  filter(!word %in% stop_words$word,
         !word %in% potter_names$firstname,
         !word %in% potter_names$lastname,
         !word %in% potter_spells$first_word,
         !word %in% potter_spells$second_word,
         !is.na(word)) |>
  group_by(title) |>
  count(word) |>
  cast_dtm(title, word, n)

# set a seed so that the output of the model is predictable
potter_lda <- LDA(potter_dtm, k = 2, control = list(seed = 1234))

potter_topics <- tidy(potter_lda, matrix = "beta")
potter_topics

# # Find the most common words within each topic
# potter_top_terms <- potter_topics |>
#   group_by(topic) |>
#   slice_max(beta, n = 10) |> 
#   ungroup() |>
#   arrange(topic, -beta)

# potter_top_terms |>
#   mutate(term = reorder_within(term, beta, topic)) |>
#   ggplot(aes(beta, term, fill = factor(topic))) +
#     geom_col(show.legend = FALSE) +
#     facet_wrap(~ topic, scales = "free") +
#     scale_y_reordered()

# Find words with greatest difference between two topics, using log ratio
potter_beta_wide <- potter_topics |>
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |> 
  filter(topic1 > .001 | topic2 > .001) |>
  mutate(log_ratio = log2(topic2 / topic1))

potter_beta_wide

potter_beta_wide |>
  arrange(desc(abs(log_ratio))) |>
  slice_max(abs(log_ratio), n = 20) |>
  mutate(term = reorder(term, log_ratio)) |>
  ggplot(aes(log_ratio, term, fill = log_ratio > 0)) +
    geom_col(show.legend = FALSE) +
    labs(x = "Log ratio of Beta values",
         y = "Words in Potter Books")

potter_documents <- tidy(potter_lda, matrix = "gamma")

# Find documents for each topic
potter_documents |>
  group_by(topic) |>
  slice_max(gamma, n = 10) |>
  ungroup() |>
  arrange(topic, -gamma)
```

>> Topic 1: Chamber of Secrests and Prisoner of Azkaban, Topic 2: Half-Blood Prince, Sorcerer's Stone, Order of the Phoenix, Goblet of Fire and Deathly Hallows 
