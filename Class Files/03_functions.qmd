---
title: "Ziling Zhen Functions and tidy evaluation"
format:
  html: default
  pdf: default
editor_options: 
  chunk_output_type: console
---

Based on Chapter 25 from *R for Data Science*

You can download this .qmd file from [here](https://github.com/proback/264_fall_2024/blob/main/03_functions.qmd).  Just hit the Download Raw File button.

### Introduction (from Ch 25 of R4DS)

One of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has four big advantages over using copy-and-paste:

  - You can give a function an evocative name that makes your code easier to understand.
  - As requirements change, you only need to update code in one place, instead of many.
  - You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).
  - It makes it easier to reuse work from project-to-project, increasing your productivity over time.

A good rule of thumb is to consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). We’ll learn about three useful types of functions:

  - Vector functions take one or more vectors as input and return a vector as output.
  - Data frame functions take a data frame as input and return a data frame as output.
  - Plot functions that take a data frame as input and return a plot as output.

```{r}
#| message: false
#| warning: false

# Initial packages required (we'll be adding more)
library(tidyverse)
library(nycflights13)
```


**Do not Repeat Yourself**: Also known as DRY, if you copy or paste code more than twice, you should write a function instead.

When writing a function, it is usually best to start with the code you know works for one instance, and then "function-ize" it.


## Vector functions

### Example 1: Rescale variables from 0 to 1.

This code creates a 10 x 4 tibble filled with random values taken from a normal distribution with mean 0 and SD 1

```{r}
#| message: false
#| warning: false

df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
df
```

This code below for rescaling variables from 0 to 1 is ripe for functions... we did it four times!

**It's easiest to start with working code and turn it into a function.**

```{r}
df$a <- (df$a - min(df$a)) / (max(df$a) - min(df$a))
df$b <- (df$b - min(df$b)) / (max(df$b) - min(df$b))
df$c <- (df$c - min(df$c)) / (max(df$c) - min(df$c))
df$d <- (df$d - min(df$d)) / (max(df$d) - min(df$d))
df
```

Notice first what changes and what stays the same in each line.  Then, if we look at the first line above, we see we have one value we're using over and over: `df$a`.  So our function will have one input. We'll start with our code from that line, then replace the input (df$a) with x. We should give our function a name that explains what it does.  The name should be a verb.

```{r}
# I'm going to show you how to write the function in class! 
# I have it in the code already below, but don't look yet!
# Let's try to write it together first!
rescale01 <- function(x){
  (x - min(x)) / (max(x) - min(x)) 
}

rescale01(df$a)
```
.
.
.
.
.
.
.
.
.

```{r}
# Our function (first draft!)
rescale01 <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
```

Note the **general form of a function**:

```{r}
#| eval: FALSE

name <- function(arguments) {
  body
}
```

Every function contains 3 essential components:

  - A name. The name should clearly evoke what the function does; hence, it is often a verb (action).  Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1.  snake_case is good; CamelCase is just okay.
  - The arguments. The arguments are things that vary across calls and they are usually nouns - first the data, then other details.  Our analysis above tells us that we have just one; we’ll call it x because this is the conventional name for a numeric vector, but you can use any word.
  - The body. The body is the code that’s repeated across all the calls.  By default a function will return the last statement; use `return()` to specify a return value

**Summary:** Functions should be written for both humans and computers!


Once we have written a function we like, then we need to test it with different inputs!

```{r}
temp <- c(4, 6, 8, 9)
rescale01(temp)

temp0 <- c(4, 6, 8, 9, NA)
rescale01(temp0)
```

OK, so NA's don't work the way we want them to.  

```{r}
rescale01 <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
rescale01(temp)
rescale01(temp0)
```

We can continue to improve our function.  Here is another method, which uses the existing `range` function within R to avoid 3 max/min executions:

```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
rescale01(temp)
rescale01(c(0, 5, 10))
rescale01(c(-10, 0, 10))
rescale01(c(1, 2, 3, NA, 5))
```

We should continue testing unusual inputs.  Think carefully about how you want this function to behave... the current behavior is to include the Inf (infinity) value when calculating the range.  You get strange output everywhere, but it's pretty clear that there is a problem right away when you use the function.  In the example below (rescale1), you ignore the infinity value when calculating the range.  The function returns Inf for one value, and sensible stuff for the rest.  In many cases this may be useful, but it could also hide a problem until you get deeper into an analysis.

```{r}
x <- c(1:10, Inf)
rescale01(x)
rescale1 <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
rescale1(x)
```

Now we've used functions to simplify original example. We will learn to simplify further in iterations (Ch 26)

```{r}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
# add a little noise
df$a[5] = NA
df$b[6] = Inf
df

df$a_new <- rescale1(df$a)
df$b_new <- rescale1(df$b)
df$c_new <- rescale1(df$c)
df$d_new <- rescale1(df$d)
df

df %>% 
  mutate(a_new = rescale1(a),
         b_new = rescale1(b),
         c_new = rescale1(c),
         d_new = rescale1(d))

# Even better - from Chapter 26
df |> mutate(across(a:d, rescale1))
```


### Options for handling NAs in functions

Before we try some practice problems, let's consider various options for handling NAs in functions.  We used the `na.rm` option within functions like `min`, `max`, and `range` in order to take care of missing values.  But there are alternative approaches:

* filter/remove the NA values before rescaling
* create an if statement to check if there are NAs; return an error if NAs exist
* create a removeNAs option in the function we are creating

Let's take a look at each alternative approach in turn:


#### Filter/remove the NA values before rescaling

```{r}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
df$a[5] = NA
df

rescale_basic <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

df %>%
  filter(!is.na(a)) %>%
  mutate(new_a = rescale_basic(a))
```

**[Pause to Ponder:]** Do you notice anything in the output above that gives you pause?  

>> Questioning if it is necessary to remove the entire row because if we wanted to calculate a new_b in the same mutate, we wouldn't have those weighted values in row 5.

#### Create an if statement to check if there are NAs; return an error if NAs exist

First, here's an example involving weighted means:

```{r}
# Create function to calculate weighted mean
wt_mean <- function(x, w) {
  sum(x * w) / sum(w)
}
wt_mean(c(1, 10), c(1/3, 2/3))
wt_mean(1:6, 1:3)
```


**[Pause to Ponder:]** Why is the answer to the last call above 7.67?  Aren't we taking a weighted mean of 1-6, all of which are below 7?

>> It repeats the list of 1, 2, and 3 so it matches the same size as 1:6, so in theory the w vector will repear itself to be 1, 2, 3, 1, 2, 3. Thus when it calculates the wt_mean function it will calculate 1 + 4 + + 4 + 10 + 18 = 46, then since that is the sum(x * w) then it will divide it by the sum(w) in our case is 1+2+3, thus we have 46 / 6 which is equal to 7.666667.

```{r}
#| error: TRUE

# update function to handle cases where data and weights of unequal length
wt_mean <- function(x, w) {
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  } else {
  sum(w * x) / sum(w)
  }  
}
wt_mean(1:6, 1:3) 
wt_mean(1:6, 1:6) 
# should produce an error now if weights and data different lengths
#  - nice example of if and else
```

**[Pause to Ponder:]** What does the `call.` option do?

>> For call. = TRUE when an error happens it will say "Error in wt_mean(1:6, 1:3)", then give out your error message. For call. = FALSE when an error happens it will say "Error:" and give you your error message. Basically indicates if the function you're trying to run will be in the error message in our case this is wt_mean(1:6, 1:3) 

Now let's apply this to our rescaling function

```{r}
#| error: TRUE

rescale_w_error <- function(x) {
  if (is.na(sum(x))) {
    stop("`x` cannot have NAs", call. = FALSE)
  } else {
  (x - min(x)) / (max(x) - min(x))
  }  
}

temp <- c(4, 6, 8, 9)
rescale_w_error(temp)

temp <- c(4, 6, 8, 9, NA)
rescale_w_error(temp)
```

**[Pause to Ponder:]** Why can't we just use `if (is.na(x))` instead of `is.na(sum(x))`?

>> It returns a vector instead of a scalar. The If needs a single value that can be either true or
false.


#### Create a removeNAs option in the function we are creating

```{r}
rescale_NAoption <- function(x, removeNAs = FALSE) {
  (x - min(x, na.rm = removeNAs)) / 
    (max(x, na.rm = removeNAs) - min(x, na.rm = removeNAs))
} 

temp <- c(4, 6, 8, 9)
rescale_NAoption(temp)

temp <- c(4, 6, 8, 9, NA)
rescale_NAoption(temp, removeNAs = TRUE)
```

OK, but all the other summary stats functions use na.rm as the input, so to be consistent, it's probably better to do something slightly awkward like this:

```{r}
rescale_NAoption <- function(x, na.rm = FALSE) {
  (x - min(x, na.rm = na.rm)) / 
    (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))
} 

temp <- c(4, 6, 8, 9, NA)
rescale_NAoption(temp, na.rm = TRUE)
```


`wt_mean()` is an example of a "summary function (single value output) instead of a "mutate function" (vector output) like `rescale01()`.  Here's another summary function to produce the mean absolute percentage error:

```{r}
mape <- function(actual, predicted) {
  sum(abs((actual - predicted) / actual)) / length(actual)
}

y <- c(2,6,3,8,5)
yhat <- c(2.5, 5.1, 4.4, 7.8, 6.1)
mape(actual = y, predicted = yhat)
```


## Data frame functions

These work like dplyr verbs, taking a data frame as the first argument, and then returning a data frame or a vector.

### Demonstration of tidy evaluation in functions

```{r}
#| error: TRUE

# Start with working code then functionize
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point(size = 0.75) +
  geom_smooth()

make_plot <- function(dataset, xvar, yvar, pt_size = 0.75)  {
  ggplot(data = dataset, mapping = aes(x = xvar, y = yvar)) +
    geom_point(size = pt_size) +
    geom_smooth()
}

make_plot(dataset = mpg, xvar = cty, yvar = hwy)  # Error!
```

The problem is tidy evaluation, which makes most common coding easier, but makes some less common things harder.  Key terms to understand tidy evaluation:

  - env-variables = live in the environment (mpg)
  - data-variables = live in data frame or tibble (cty)
  - data masking = tidyverse use data-variables as if they are env-variables.  That is, you don't always need `mpg$cty` to access `cty` in tidyverse
  
The key idea behind data masking is that it blurs the line between the two different meanings of the word “variable”:

  - env-variables are “programming” variables that live in an environment. They are usually created with <-.
  - data-variables are “statistical” variables that live in a data frame. They usually come from data files (e.g. .csv, .xls), or are created manipulating existing variables.

The solution is to embrace {{ }} data-variables which are user inputs into functions.  One way to remember what’s happening, as suggested by our book authors, is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of `var` rather than looking for a variable called `var`.  Thus, embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name.

See Section 25.3 of R4DS for more details (and there are plenty!).

```{r}
# This will work to make our plot!
make_plot <- function(dataset, xvar, yvar, pt_size = 0.75)  {
  ggplot(data = dataset, mapping = aes(x = {{ xvar }}, y = {{ yvar }})) +
    geom_point(size = pt_size) +
    geom_smooth()
}

make_plot(dataset = mpg, xvar = cty, yvar = hwy)
```

I often wish it were easier to get my own custom summary statistics for numeric variables in EDA rather than using `mosaic:favstats()`.  Using `group_by()` and `summarise()` from the tidyverse reads clearly but takes so many lines, but if I only had to write the code once...

```{r}
summary6 <- function(data, var) {
  data |> summarize(
    min = min({{ var }}, na.rm = TRUE),
    mean = mean({{ var }}, na.rm = TRUE),
    median = median({{ var }}, na.rm = TRUE),
    max = max({{ var }}, na.rm = TRUE),
    n = n(),
    n_miss = sum(is.na({{ var }})),
    .groups = "drop"    # to leave the data in an ungrouped state
  )
}

mpg |> summary6(hwy)
```

Even cooler, I can use my new function with `group_by()`!

```{r}
mpg |> 
  group_by(drv) |>
  summary6(hwy)
```

You can even pass conditions into a function using the embrace:

**[Pause to Ponder:]** Predict what the code below will do, and (only) then run it to check.  Think about: why do we have `sort = sort`?  why not embrace `df`? why didn't we need `n` in the arguments?

>> It will creates a new function, that filters by the specified condition, counts the variable and
then finds the proportion for each variable. sort = sort, sort on the left hand is the option in count, and the sort on the right hand side is our name for it. So in our function it'll output it in numerical order.

We didn't need n in the argument because it is being created in our count.

```{r}
#| eval: FALSE

new_function <- function(df, var, condition, sort = TRUE) {
  df |>
    filter({{ condition }}) |>
    count({{ var }}, sort = sort) |>
    mutate(prop = n / sum(n))
}

mpg |> new_function(var = manufacturer, 
                    condition = manufacturer %in% c("audi", "honda", "hyundai", "nissan", "subaru", "toyota", "volkswagen"))
```

### Data-masking vs. tidy-selection (Section 25.3.4)

Why doesn't the following code work?

```{r}
#| error: TRUE

count_missing <- function(df, group_vars, x_var) {
  df |> 
    group_by({{ group_vars }}) |> 
    summarize(
      n_miss = sum(is.na({{ x_var }})),
      .groups = "drop"
    )
}

flights |> 
  count_missing(c(year, month, day), dep_time)
```

The problem is that `group_by()` uses data-masking rather than tidy-selection; it is selecting certain variables rather than evaluating values of those variables.  These are the two most common subtypes of tidy evaluation:

  - Data-masking is used in functions like arrange(), filter(), mutate(), and summarize() that compute with variables.  Data masking is an R feature that blends programming variables that live inside environments (env-variables) with statistical variables stored in data frames (data-variables).  
  - Tidy-selection is used for functions like select(), relocate(), and rename() that select variables.  Tidy selection provides a concise dialect of R for selecting variables based on their names or properties.
  
More detail can be found [here](https://dplyr.tidyverse.org/articles/programming.html).

The error above can be solved by using the `pick()` function, which uses tidy selection inside of data masking:

```{r}
count_missing <- function(df, group_vars, x_var) {
  df |> 
    group_by(pick({{ group_vars }})) |> 
    summarize(
      n_miss = sum(is.na({{ x_var }})),
      .groups = "drop"
  )
}

flights |> 
  count_missing(c(year, month, day), dep_time)
```

**[Pause to Ponder:]** Here's another nice use of `pick()`.  Predict what the function will do, then run the code to see if you are correct.

>> It will choose those the rows and colums we want to count, then take the values of column in our pivot wider is a differetn way of organizing counts, we will have manufacturer, model, and then columns with the number of cylinders  

```{r}
#| eval: FALSE

# Source: https://twitter.com/pollicipes/status/1571606508944719876
new_function <- function(data, rows, cols) {
  data |> 
    count(pick(c({{ rows }}, {{ cols }}))) |> 
    pivot_wider(
      names_from = {{ cols }}, 
      values_from = n,
      names_sort = TRUE,
      values_fill = 0
    )
}

mpg |> new_function(c(manufacturer, model), cyl)
```


## Plot functions

Let's say you find yourself making a lot of histograms:

```{r}
#| warning: FALSE

flights |> 
  ggplot(aes(x = dep_time)) +
  geom_histogram(bins = 25)

flights |> 
  ggplot(aes(x = air_time)) +
  geom_histogram(bins = 35)
```

Just use embrace to create a histogram-making function

```{r}
#| warning: FALSE

histogram <- function(df, var, bins = NULL) {
  df |> 
    ggplot(aes(x = {{ var }})) + 
    geom_histogram(bins = bins)
}

flights |> histogram(air_time, 35)
```

Since histogram() returns a ggplot, you can add any layers you want

```{r}
#| warning: FALSE

flights |> 
  histogram(air_time, 35) +
  labs(x = "Flight time (minutes)", y = "Number of flights")
```

You can also combine data wrangling with plotting.  Note that we need the "walrus operator" (:=) since the variable name on the left is being generated with user-supplied data.

```{r}
# sort counts with highest values at top and counts on x-axis
sorted_bars <- function(df, var) {
  df |> 
    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |>
    ggplot(aes(y = {{ var }})) +
    geom_bar()
}

flights |> sorted_bars(carrier)
```

Finally, it would be really helpful to label plots based on user inputs.  This is a bit more complicated, but still do-able!

For this, we'll need the `rlang` package.  `rlang` is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).

Check out the following update of our `histogram()` function which uses the `englue()` function from the `rlang` package:

```{r}
#| warning: FALSE

histogram <- function(df, var, bins) {
  label <- rlang::englue("A histogram of {{var}} with binwidth {bins}")
  
  df |> 
    ggplot(aes(x = {{ var }})) + 
    geom_histogram(bins = bins) + 
    labs(title = label)
}

flights |> histogram(air_time, 35)
```


### On Your Own

1. Rewrite this code snippet as a function: `x / sum(x, na.rm = TRUE)`.  This code creates weights which sum to 1, where NA values are ignored. Test it for at least two different vectors. (Make sure at least one has NAs!)

```{r}
weight_sum <- function(x) {
  x / sum(x, na.rm = TRUE)
}

temp1 <- c(2, 4, 6, 8, 10)
temp2 <- c(2, 4, 6, 8, 10, NA)

weight_sum(temp2)
```


2. Create a function to calculate the standard error of a variable, where SE = square root of the variance divided by the sample size.  Hint: start with a vector like `x <- 0:5` or `x <- gss_cat$age` and write code to find the SE of x, then turn it into a function to handle any vector `x`. Note: `var` is the function to find variance in R and `sqrt` does square root. `length` may also be handy. Test your function on two vectors that do not include NAs  (i.e. do **not** worry about removing NAs at this point).

```{r}
standard_e <- function(x){
  length = length(x)
  sd = sd(x)
  sd/sqrt(length)
}

standard_e(x <- 0:5)
```


3. Use your `se` function within summarize to get a table of the mean and s.e. of `hwy` and `cty` by `class` in the `mpg` dataset.

```{r}
mpg |>
  group_by(class) |>
  summarize(mean_cty = mean(cty),
            se_cty = standard_e(cty),
            mean_hwy = mean(hwy),
            se_hwy = standard_e(hwy))
```


4. Use your `se` function within summarize to get a table of the mean and s.e. of `arr_delay` and `dep_delay` by carrier in the `flights` dataset. Why does the output look like this?

```{r}
flights |>
  group_by(carrier) |>
  summarize(mean_arr_delay = mean(arr_delay),
            se_arr_delay = standard_e(arr_delay),
            mean_dep_delay = mean(dep_delay),
            se_dep_delay = standard_e(dep_delay))
```

>> It looks like this because we have NA's for arr_delay and dep_delay and our function isn't built/designed to handle NA's. 

5. Make your `se` function handle NAs with an na.rm option. Test your new function (you can call it `se` again) on a vector that doesn't include NA and on the same vector with an added NA. **Be sure to check that it gives the expected output with na.rm = TRUE and na.rm = FALSE.** Make na.rm = FALSE the default value. Repeat #4.  (Hint: be sure when you divide by sample size you don't count any NAs)

```{r}

standard_e <- function(x, na.rm = TRUE){
  length = length(x) - sum(is.na(x))
  sd = sd(x, na.rm = TRUE)
  sd/sqrt(length)
}

test <- c(0:5, NA)

standard_e(test, na.rm = TRUE)

flights |>
  drop_na(arr_delay, dep_delay) |>
  group_by(carrier) |>
  summarize(mean_arr_delay = mean(arr_delay, na.rm = TRUE),
            se_arr_delay = standard_e(arr_delay, na.rm = TRUE),
            mean_dep_delay = mean(dep_delay, na.rm = TRUE),
            se_dep_delay = standard_e(dep_delay, na.rm = TRUE))
```


6. Create `both_na()`, a function that takes two vectors of the same length and returns how many positions have an NA in both vectors.  Hint: create two vectors like `test_x <- c(1, 2, 3, NA, NA)` and `test_y <- c(NA, 1, 2, 3, NA)` and write code that works for `test_x` and `test_y`, then turn it into a function that can handle any `x` and `y`.  (In this case, the answer would be 1, since both vectors have NA in the 5th position.)  Test it for at least one more combination of `x` and `y`.

```{r}
test_x <- c(1, 2, 3, NA, NA)
test_y <- c(NA, 1, 2, 3, NA)

both_na <- function(x, y){
  sum(is.na(x) & is.na(y))
}

both_na(test_x, test_y)
```


7. Run your code from (6) with the following two vectors: `test_x <- c(1, 2, 3, NA, NA, NA)` and `test_y <- c(NA, 1, 2, 3, NA)`. Did you get the output you wanted or expected?  Modify your function using `if`, `else`, and `stop` to print an error if x and y are not the same length.  Then test again with `test_x`, `test_y` and the sets of vectors you used in (6).

```{r, error = TRUE}
test_x2 <- c(1, 2, 3, NA, NA, NA)
test_y2 <- c(NA, 1, 2, 3, NA)

bothna2 <- function(x,y){
if (length(x) != length(y)) {
    stop("`x` and `y` ARE NOT THE SAME LENGTH, the same pls", call. = FALSE)
  } else {
  sum(is.na(x) & is.na(y))
  }  
}

bothna2(test_x2, test_y2)
bothna2(test_x, test_y)
```


8. Here is a way to get `not_cancelled` flights in the flights dataset:

```{r}
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))
```

Is it necessary to check is.na for both departure and arrival? Using summarize, find the number of flights missing departure delay, arrival delay, and both. (Use your new function!)

```{r}
flights |>
  summarize(dep_na = sum(is.na(dep_delay)),
            arr_na = sum(is.na(arr_delay)),
            both_na = bothna2(dep_delay, arr_delay))
```

>> Is it not necessary to check is.na forth both departure and arrival, we could only check for departure assuming that they're the same flights. However, there are more NA values for arr_na than dep_na.

9. Read the code for each of the following three functions, puzzle out what they do, and then brainstorm better names.

```{r}
#| eval: FALSE

duration_mins <- function(time1, time2) {
  hour1 <- time1 %/% 100
  min1 <- time1 %% 100
  hour2 <- time2 %/% 100
  min2 <- time2 %% 100
  
  (hour2 - hour1)*60 + (min2 - min1)
}


area_inches <- function(lengthcm, widthcm) {
  (lengthcm / 2.54) * (widthcm / 2.54)
}


non_answer <- function(x) {
  fct_collapse(x, "non answer" = c("No answer", "Refused", 
                                   "Don't know", "Not applicable"))
}

```

>> f1, function to calculate duration of event (like flights in our flights dataset), will output out a time in minutes. This function takes in two inputs being time EXPLAIN LATER REMEMBER 

>> f2, function that calculates area of a square or rectangle/something with 2 sides with inputs being in centimeters, and output being inches

>> f3 function for looking at a vector/column and if the answers are "No answer", "Refused", "Don't know", "Not applicable" then it becomes non answer.

10. Explain what the following function does and demonstrate by running `foo1(x)` with a few appropriately chosen vectors `x`.  (Hint: set x and run the "guts" of the function piece by piece.)

```{r, error = TRUE}

foo1 <- function(x) {
  diff <- x[-1] - x[1:(length(x) - 1)]
  sum(diff < 0)
}

```

>> The function takes an input vector x and then slices the first number out so we have the every number after the first number then subtracts it from everything but the last number. For ex. the 2nd number minus the 1st and then the 3rd minus the 2nd and 4th minus the 3rd etc. until there are no more numbers in the list. It will then assign that to diff, if the sum of diff is less than zero then it will print out a 0 and if none of those differences are less than zero it will spit out a zero, if it is less than zero then it will give the number of digits in the vector that are less than zero. 

11. The `foo1()` function doesn't perform well if a vector has missing values.  Amend `foo1()` so that it produces a helpful error message and stops if there are any missing values in the input vector.  Show that it works with appropriately chosen vectors `x`.  Be sure you add `error = TRUE` to your R chunk, or else knitting will fail!

```{r, error = TRUE}
foo1 <- function(x) {
  if(any(is.na(x))) {
stop ("Input Vector Contains NA Values", call. = FALSE)
}
diff <- x[-1] - x[1:(length(x) - 1)]
sum(diff < 0)
}

temp0

foo1(temp0)

```


12. Write a function called `greet` using `if`, `else if`, and `else` to print out "good morning" if it's before 12 PM, "good afternoon" if it's between 12 PM and 5 PM, and "good evening" if it's after 5 PM.  Your function should work if you input a time like: `greet(time = "2018-05-03 17:38:01 CDT")` or if you input the current time with `greet(time = Sys.time())`.  [Hint: check out the `hour` function in the `lubridate` package]

```{r}
hour("2018-05-03 23:38:01 CDT")

greet <- function(time){
  hour = hour(time)
if (hour < 12) {
    print("good morning")
  } else if (hour >= 12 & hour < 17){
    print("good afternoon")
  } else {
    print("good evening")
  }
}

greet(time = Sys.time())
```


13. Modify the `summary6()` function from earlier to add an argument that gives the user an option to remove missing values, if any exist.  Show that your function works for (a) the `hwy` variable in `mpg_tbl <- as_tibble(mpg)`, and (b) the `age` variable in `gss_cat`.

```{r}
summary6 <- function(data, var, na.rm = TRUE) {
  data |> summarize(
    min = min({{ var }}, na.rm = na.rm),
    mean = mean({{ var }}, na.rm = na.rm),
    median = median({{ var }}, na.rm = na.rm),
    max = max({{ var }}, na.rm = na.rm),
    n = n(),
    n_miss = sum(is.na({{ var }})),
    .groups = "drop"    # to leave the data in an ungrouped state
  )
}

mpg |> 
  summary6(hwy, na.rm = FALSE)

gss_cat |>
  summary6(age, na.rm = FALSE)

gss_cat |>
  summary6(age, na.rm = TRUE)

```


14. Add an argument to (13) to produce summary statistics by group for a second variable (you should now have 4 possible inputs to your function).  Show that your function works for (a) the `hwy` variable in `mpg_tbl <- as_tibble(mpg)` grouped by `drv`, and (b) the `age` variable in `gss_cat` grouped by `partyid`.

```{r}
summary6 <- function(data, group_var, var, na.rm = TRUE) {
  data |> 
    group_by(pick({{ group_var }})) |>
    summarize(
    min = min({{ var }}, na.rm = na.rm),
    mean = mean({{ var }}, na.rm = na.rm),
    median = median({{ var }}, na.rm = na.rm),
    max = max({{ var }}, na.rm = na.rm),
    n = n(),
    n_miss = sum(is.na({{ var }})),
    sd = sd({{ var }}, na.rm = na.rm),
    iqr = sd({{ var}}, na.rm = na.rm),
    .groups = "drop"    # to leave the data in an ungrouped state
  )
}

summary6(mpg, drv, hwy)

new_party_id <- summary6(gss_cat, partyid, age)
```


15. Create a function that has a vector as the input and returns the last value. (Note: Be sure to use a name that does not write over an existing function!)

```{r}
last_value <- function(x){
  last_num <- length(x)
  x[last_num]
}

temp
last_value(temp)

temp1
last_value(temp1)
```


16. Save your final table from (14) and write a function to draw a scatterplot of a measure of center (mean or median - user can choose) vs. a measure of spread (sd or IQR - user can choose), with points sized by sample size, to see if there is constant variance.  Each point should be labeled with partyid, and the plot title should reflect the variables chosen by the user.

```{r}
library(ggrepel)
# Testing out what I need for my function
# new_party_id |>
# ggplot(aes(x = mean, y = sd)) +
#   geom_point(aes(size = n)) +
#   geom_smooth(method = lm, se = FALSE) +
#   geom_label_repel(aes(label = partyid)) +
#   labs(title = "title")
```

```{r}
plot_summary6 <- function(tbl, center, spread) {
 label <- rlang::englue("A scatterplot of {{center}} vs {{spread}} with size being sample size")
 tbl |>
   ggplot(aes(x = {{center}}, y = {{spread}} )) +
  geom_point(aes(size = n)) +
  geom_smooth(method = lm, se = FALSE) +
  geom_label_repel(aes(label = partyid)) +
  labs(title = label)
}

plot_summary6(new_party_id, median, iqr)
plot_summary6(new_party_id, mean, sd)
```

