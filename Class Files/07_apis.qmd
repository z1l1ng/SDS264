---
title: "Data Acquisition with APIs in R"
format:
  html: default
editor_options: 
  chunk_output_type: console
---
  
You can download this .qmd file from [here](https://github.com/proback/264_fall_2024/blob/main/07_apis.qmd).  Just hit the Download Raw File button.

Credit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(httr2)
library(httr)
```


## Getting data from websites

# Option 1: APIs

When we interact with sites like The New York Times, Zillow, and Google, we are accessing their data via a graphical layout (e.g., images, colors, columns) that is easy for humans to read but hard for computers.

An **API** stands for **Application Programming Interface**, and this term describes a general class of tool that allows computers, rather than humans, to interact with an organization's data. How does this work?

- When we use web browsers to navigate the web, our browsers communicate with web servers using a technology called [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) or Hypertext Transfer Protocol to get information that is formatted into the display of a web page.
- Programming languages such as R can also use HTTP to communicate with web servers. The easiest way to do this is via [Web APIs](https://en.wikipedia.org/wiki/Web_API), or Web Application Programming Interfaces, which focus on transmitting raw data, rather than images, colors, or other appearance-related information that humans interact with when viewing a web page.

A large variety of web APIs provide data accessible to programs written in R (and almost any other programming language!). Almost all reasonably large commercial websites offer APIs. Todd Motto has compiled an expansive list of [Public Web APIs](https://github.com/toddmotto/public-apis) on GitHub, although it's about 3 years old now so it's not a perfect or complete list. Feel free to browse this list to see what data sources are available.

For our purposes of obtaining data, APIs exist where website developers make data nicely packaged for consumption.  The language HTTP (hypertext transfer protocol) underlies APIs, and the R package `httr()` (and now the updated `httr2()`) was written to map closely to HTTP with R. Essentially you send a request to the website (server) where you want data from, and they send a response, which should contain the data (plus other stuff).

The case studies in this document provide a really quick introduction to data acquisition, just to get you started and show you what's possible.  For more information, these links can be somewhat helpful:

- https://towardsdatascience.com/functions-with-r-and-rvest-a-laymens-guide-acda42325a77
- https://nceas.github.io/oss-lessons/data-liberation/intro-webscraping.html


## Wrapper packages

In R, it is easiest to use Web APIs through a **wrapper package**, an R package written specifically for a particular Web API.

- The R development community has already contributed wrapper packages for many large Web APIs (e.g. ZillowR, rtweet, genius, Rspotify, tidycensus, etc.)
- To find a wrapper package, search the web for "R package" and the name of the website. For example:
    - Searching for "R Reddit package" returns [RedditExtractor](https://github.com/ivan-rivera/RedditExtractor)
    - Searching for "R Weather.com package" returns [weatherData](https://ram-n.github.io/weatherData/)
- [rOpenSci](https://ropensci.org/packages/) also has a good collection of wrapper packages.

In particular, `tidycensus` is a wrapper package that makes it easy to obtain desired census information for mapping and modeling:

```{r}
#| message: FALSE
#| echo: FALSE

# Grab total population and median household income for all
#   census tracts in MN using Census Bureau API
library(tidycensus)
sample_acs_data <- tidycensus::get_acs(
    year = 2020,
    state = "MN",
    geography = "tract",
    variables = c("B01003_001", "B19013_001"),
    output = "wide",
    geometry = TRUE
)
```

Obtaining raw data from the Census Bureau was that easy!  Often we will have to obtain and use a secret API key to access the data, but that's not always necessary with `tidycensus`.

Now we can tidy that data and produce plots and analyses.

```{r}
#| warning: FALSE
#| message: FALSE

# Rename cryptic variables from the census form
sample_acs_data <- sample_acs_data |>
  rename(population = B01003_001E,
         population_moe = B01003_001M,
         median_income = B19013_001E,
         median_income_moe = B19013_001M)

# Plot with geom_sf since our data contains 1 row per census tract
#   with its geometry
ggplot(data = sample_acs_data) + 
  geom_sf(aes(fill = median_income), colour = "white", linetype = 2) + 
  theme_void()  

# The whole state of MN is overwhelming, so focus on a single county
sample_acs_data |>
  filter(str_detect(NAME, "Hennepin")) |>
  ggplot() + 
    geom_sf(aes(fill = median_income), colour = "white", linetype = 2)

# Look for relationships between variables with 1 row per tract
as_tibble(sample_acs_data) |>
  ggplot(aes(x = population, y = median_income)) + 
    geom_point() + 
    geom_smooth(method = "lm")  
```


Extra resources:

- `tidycensus`: wrapper package that provides an interface to a few census datasets *with map geometry included!*
    - Full documentation is available at <https://walker-data.com/tidycensus/>

- `censusapi`: wrapper package that offers an interface to all census datasets
    - Full documentation is available at <https://www.hrecht.com/censusapi/>

`get_acs()` is one of the functions that is part of `tidycensus`.  Let's explore what's going on behind the scenes with `get_acs()`...


## Accessing web APIs directly

### Getting a Census API key

Many APIs (and their wrapper packages) require users to obtain a **key** to use their services.

- This lets organizations keep track of what data is being used.
- It also **rate limits** their API and ensures programs don't make too many requests per day/minute/hour. Be aware that most APIs do have rate limits --- especially for their free tiers.

Navigate to <https://api.census.gov/data/key_signup.html> to obtain a Census API key:

- Organization: St. Olaf College
- Email: Your St. Olaf email address

You will get the message:

> Your request for a new API key has been successfully submitted. Please check your email. In a few minutes you should receive a message with instructions on how to activate your new key.

Check your email. Copy and paste your key into a new text file:

- File > New File > Text File (towards the bottom of the menu)
- Save as `census_api_key.txt` in the same folder as this `.qmd`.

You could then read in the key with code like this:

```{r}
# myapikey <- readLines("00_Folder/CensusAPI.txt")
```

### Handling API keys

While this works, the problem is once we start backing up our files to GitHub, your API key will also appear on GitHub, and you want to keep your API key secret.  Thus, we might use **environment variables** instead:

One way to store a secret across sessions is with environment variables. Environment variables, or envvars for short, are a cross platform way of passing information to processes.  For passing envvars to R, you can list name-value pairs in a file called .Renviron in your home directory. The easiest way to edit it is to run:

```{r}
#| eval: FALSE

file.edit("~/.Renviron")
```

The file looks something like

PATH = "path"
VAR1 = "value1"
VAR2 = "value2"
And you can access the values in R using `Sys.getenv()`:

```{r}
#| eval: FALSE

Sys.getenv("censusapi")
#> [1] "value1"
```

Note that .Renviron is only processed on startup, so youâ€™ll need to restart R to see changes.

Another option is to use `Sys.setenv` and `Sys.getenv`:

```{r}
# I used the first line to store my CENSUS API key in .Renviron
#   after uncommenting - should only need to run one time
# Sys.setenv("CENSUS_KEY" = "my census api key pasted here")
# my_census_api_key <- Sys.getenv("CENSUS_KEY")
```

```{r}
my_census_api_key <- Sys.getenv("censusapi")
```


### Navigating API documentation

Navigate to the [Census API user guide](https://www.census.gov/data/developers/guidance/api-user-guide.html) and click on the "Example API Queries" tab.

Let's look at the Population Estimates Example and the American Community Survey (ACS) Example. These examples walk us through the steps to incrementally build up a URL to obtain desired data. This URL is known as a web API **request**. 

https://api.census.gov/data/2019/acs/acs1?get=NAME,B02015_009E,B02015_009M&for=state:*

- `https://api.census.gov`: This is the **base URL**.
    - `http://`: The **scheme**, which tells your browser or program how to communicate with the web server. This will typically be either `http:` or `https:`.
    - `api.census.gov`: The **hostname**, which is a name that identifies the web server that will process the request.
- `data/2019/acs/acs1`: The **path**, which tells the web server how to get to the desired resource.
    - In the case of the Census API, this locates a desired dataset in a particular year.
    - Other APIs allow search functionality. (e.g., News organizations have article searches.) In these cases, the path locates the search function we would like to call.
- `?get=NAME,B02015_009E,B02015_009M&for=state:*`: The **query parameters**, which provide the parameters for the function you would like to call.
    - We can view this as a string of key-value pairs separated by `&`. That is, the general structure of this part is `key1=value1&key2=value2`.

key      value
----     ------
get      NAME,B02015_009E,B02015_009M
for      state:*

Typically, each of these URL components will be specified in the API documentation. Sometimes, the scheme, hostname, and path (`https://api.census.gov/data/2019/acs/acs1`) will be referred to as the **[endpoint](https://en.wikipedia.org/wiki/Web_API#Endpoints)** for the API call.

We will first use the [`httr2` package](https://httr2.r-lib.org/) to build up a full URL from its parts.

- `request()` creates an API request object using the **base URL**
- `req_url_path_append()` builds up the URL by adding path components separated by `/`
- `req_url_query()` adds the `?` separating the endpoint from the query and sets the key-value pairs in the query
    - The `.multi` argument controls how multiple values for a given key are combined.
    - The `I()` function around `"state:*"` inhibits parsing of special characters like `:` and `*`. (It's known as the "as-is" function.)
    - The backticks around `for` are needed because `for` is a reserved word in R (for for-loops). You'll need backticks whenever the key name has special characters (like spaces, dashes).
    - We can see from [here](https://www.census.gov/data/developers/guidance/api-user-guide.Help_&_Contact_Us.html) that providing an API key is achieved with `key=YOUR_API_KEY`.


```{r}
# Request total number of Hmong residents and margin of error by state
#   in 2019, as in the User Guide
CENSUS_API_KEY <- Sys.getenv("censusapi")
req <- request("https://api.census.gov") |> 
    req_url_path_append("data") |> 
    req_url_path_append("2019") |> 
    req_url_path_append("acs") |> 
    req_url_path_append("acs1") |> 
    req_url_query(get = c("NAME", "B02015_009E", "B02015_009M"), `for` = I("state:*"), key = CENSUS_API_KEY, .multi = "comma")
```

**Why would we ever use these steps instead of just using the full URL as a string?**

- To generalize this code with functions! (This is exactly what wrapper packages do.)
- To handle special characters
    - e.g., query parameters might have spaces, which need to be represented in a particular way in a URL (URLs can't contain spaces)

Once we've fully constructed our request, we can use `req_perform()` to send out the API request and get a **response**.

```{r}
#| message: FALSE

resp <- req_perform(req)
resp
```

We see from `Content-Type` that the format of the response is something called JSON. We can navigate to the request URL to see the structure of this output.

- JSON (Javascript Object Notation) is a nested structure of key-value pairs.
- We can use `resp_body_json()` to parse the JSON into a nicer format.
    - Without `simplifyVector = TRUE`, the JSON is read in as a list. 

```{r}
resp_json_list <- resp |> resp_body_json()
head(resp_json_list, 2)
resp_json_df <- resp |> resp_body_json(simplifyVector = TRUE)
head(resp_json_df)
resp_json_df <- janitor::row_to_names(resp_json_df, 1)
head(resp_json_df)
```


All right, let's try this!  First we'll grab total population and median household income for all census tracts in MN using 3 approaches

```{r}
#| message: FALSE

# First using tidycenus
library(tidycensus)
sample_acs_data <- tidycensus::get_acs(
    year = 2021,
    state = "MN",
    geography = "tract",
    variables = c("B01003_001", "B19013_001"),
    output = "wide",
    geometry = TRUE,
    county = "Hennepin",   # specify county in call
    show_call = TRUE       # see resulting query
)
```

```{r}
#| message: FALSE

# Next using httr2
req <- request("https://api.census.gov") |> 
    req_url_path_append("data") |> 
    req_url_path_append("2020") |> 
    req_url_path_append("acs") |> 
    req_url_path_append("acs5") |> 
    req_url_query(get = c("NAME", "B01003_001E", "B19013_001E"), `for` = I("tract:*"), `in` = I("state:27"), `in` = I("county:053"), key = CENSUS_API_KEY, .multi = "comma")

resp <- req_perform(req)
resp
resp_json_df <- resp |> resp_body_json(simplifyVector = TRUE)
head(resp_json_df)
resp_json_df <- janitor::row_to_names(resp_json_df, 1)
head(resp_json_df)

hennepin_httr2 <- as_tibble(resp_json_df) |>
  mutate(population = parse_number(B01003_001E),
         median_income = parse_number(B19013_001E)) |>
  select(-B01003_001E, -B19013_001E, -state, -county)
  
hennepin_httr2 |>
  ggplot(aes(x = population, y = median_income)) + 
    geom_point() + 
    geom_smooth(method = "lm")  

summary(hennepin_httr2$population)
summary(hennepin_httr2$median_income)
sort(hennepin_httr2$population)
sort(hennepin_httr2$median_income)

hennepin_httr2 <- hennepin_httr2 |>
  mutate(median_income = ifelse(median_income > 0, median_income, NA),
         population = ifelse(population > 0, population, NA))
  
hennepin_httr2 |>
  ggplot(aes(x = population, y = median_income)) + 
    geom_point() + 
    geom_smooth(method = "lm")  

# To make choropleth map by census tract, would need to download US Census
#   Bureau TIGER geometries using tigris package
```

```{r}
#| message: FALSE

# Finally using httr
url <- str_c("https://api.census.gov/data/2020/acs/acs5?get=NAME,B01003_001E,B19013_001E&for=tract:*&in=state:27&in=county:053", "&key=", CENSUS_API_KEY)
acs5 <- GET(url)
details <- content(acs5, "parsed")
# details 
details[[1]]  # variable names
details[[2]]  # list with information on 1st tract

name = character()
population = double()
median_income = double()
tract = character()

for(i in 2:330) {
  name[i-1] <- details[[i]][[1]][1]
  population[i-1] <- details[[i]][[2]][1]
  median_income[i-1] <- details[[i]][[3]][1]
  tract[i-1] <- details[[i]][[6]][1]
}
hennepin_httr <- tibble(
  name = name,
  population = parse_number(population),
  median_income = parse_number(median_income),
  tract = tract
)
```


### On Your Own

1. Write a loop to obtain the Hennepin County data from 2017-2021


2. Write a function to give choices about year, county, and variables

```{r}
acs_function <- function(year, county, vars) {
    Sys.sleep(0.5)
tidycensus::get_acs(
    year = year,
    state = "MN",
    geography = "tract",
    variables = vars,
    output = "wide",
    geometry = TRUE,
    county = county,   # specify county in call
    show_call = TRUE       # see resulting query
)  |>
    mutate(year = year)
  }

acs_function(2021, "Hennepin", c("B01003_001", "B19013_001"))
```


3. Use your function from (2) along with `map` and `list_rbind` to build a data set for Rice county for the years 2019-2021

```{r}
years <- c(2019:2021)

ricedata <- years |> purrr::map(acs_function, county = "Rice", vars = c("B01003_001", "B19013_001")) |> list_rbind()

ricedata
```


### One more example using an API key

Here's an example of getting data from a website that attempts to make imdb movie data available as an API.

Initial instructions:

- go to omdbapi.com under the API Key tab and request a free API key
- store your key as discussed earlier
- explore the examples at omdbapi.com

We will first obtain data about the movie Coco from 2017.

```{r}
#| eval: FALSE

myapikey <- Sys.getenv("omdbapi")

# Find url exploring examples at omdbapi.com
url <- str_c("http://www.omdbapi.com/?t=Coco&y=2017&apikey=", myapikey)

coco <- GET(url)   # coco holds response from server
coco               # Status of 200 is good!

details <- content(coco, "parse")   
details                         # get a list of 25 pieces of information
details$Year                    # how to access details
details[[2]]                    # since a list, another way to access
```

Now build a data set for a collection of movies

```{r}
#| message: FALSE
#| eval: FALSE

# Must figure out pattern in URL for obtaining different movies
#  - try searching for others
movies <- c("Coco", "Wonder+Woman", "Get+Out", 
            "The+Greatest+Showman", "Thor:+Ragnarok")

# Set up empty tibble
omdb <- tibble(Title = character(), Rated = character(), Genre = character(),
       Actors = character(), Metascore = double(), imdbRating = double(),
       BoxOffice = double())

# Use for loop to run through API request process 5 times,
#   each time filling the next row in the tibble
#  - can do max of 1000 GETs per day
for(i in 1:5) {
  url <- str_c("http://www.omdbapi.com/?t=",movies[i],
               "&apikey=", myapikey)
  Sys.sleep(0.5)
  onemovie <- GET(url)
  details <- content(onemovie, "parse")
  omdb[i,1] <- details$Title
  omdb[i,2] <- details$Rated
  omdb[i,3] <- details$Genre
  omdb[i,4] <- details$Actors
  omdb[i,5] <- parse_number(details$Metascore)
  omdb[i,6] <- parse_number(details$imdbRating)
  omdb[i,7] <- parse_number(details$BoxOffice)   # no $ and ,'s
}

omdb

#  could use stringr functions to further organize this data - separate 
#    different genres, different actors, etc.
```


### On Your Own (continued)

4. (Based on final project by Mary Wu and Jenna Graff, MSCS 264, Spring 2024).  Start with a small data set on 56 national parks from [kaggle](https://www.kaggle.com/datasets/nationalparkservice/park-biodiversity), and supplement with columns for the park address (a single column including address, city, state, and zip code) and a list of available activities (a single character column with activities separated by commas) from the park websites themselves.

Preliminaries:

- Request API [here](https://www.nps.gov/subjects/developer/get-started.htm)
- Check out [API guide](https://www.nps.gov/subjects/developer/guides.htm)

```{r}
np_kaggle <- read_csv("~/264_fall_2024/Data/parks.csv")
```

